{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "# Prepare train data\n",
    "train_X = np.linspace(-1, 1, 200)\n",
    "train_Y = 2 * train_X + np.random.randn(*train_X.shape) * 0.33 + 10\n",
    "plt.plot(train_X,train_Y,\"+\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The optimal goal is to learn [W, b] which is [2, 10]\n",
    "# Define the model\n",
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")\n",
    "w = tf.Variable(0.0, name=\"weight\")\n",
    "b = tf.Variable(0.0, name=\"bias\")\n",
    "loss = tf.square(Y - X*w - b) # mean square loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the optmization operation.\n",
    "# doing w = w - \\alpha * dw  (gradient descent)\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create session to run\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    epoch = 1\n",
    "    for i in range(40):\n",
    "        for (x, y) in zip(train_X, train_Y):\n",
    "            _, w_value, b_value = sess.run([train_op, w, b],feed_dict={X: x,Y: y})\n",
    "        print(\"Epoch: {}, w: {}, b: {}\".format(epoch, w_value, b_value))\n",
    "        epoch += 1\n",
    "\n",
    "\n",
    "#draw\n",
    "plt.plot(train_X,train_Y,\"+\")\n",
    "plt.plot(train_X,train_X.dot(w_value)+b_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network \n",
    "\n",
    "refs: https://github.com/nlintz/TensorFlow-Tutorials/blob/master/05_convolutional_net.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![CNN architecture](http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Typical_cnn.png)\n",
    "\n",
    "[image source](http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n",
    "trX = trX.reshape(-1, 28, 28, 1)  # 28x28x1 input img\n",
    "teX = teX.reshape(-1, 28, 28, 1)  # 28x28x1 input img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import `matplotlib`\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "%matplotlib inline \n",
    "# chose some images to display\n",
    "choosen_ind = [300, 2250, 3650, 4000]\n",
    "\n",
    "# Fill out the subplots with the random images and add shape, min and max values\n",
    "for i in range(len(choosen_ind)):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(trX[i,:,:,0])\n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "    plt.show()\n",
    "    print('label: ', np.argmax(trY[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "test_size = 256\n",
    "\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "def model(X, w, w2, w3, w4, w_o):\n",
    "    with tf.variable_scope('fist_conv_max'):\n",
    "        l1a = tf.nn.relu(tf.nn.conv2d(X, w,                       # l1a shape=(?, 28, 28, 32)\n",
    "                            strides=[1, 1, 1, 1], padding='SAME'))\n",
    "        l1 = tf.nn.max_pool(l1a, ksize=[1, 2, 2, 1],              # l1 shape=(?, 14, 14, 32)\n",
    "                            strides=[1, 2, 2, 1], padding='SAME')\n",
    "        #l1 = tf.nn.dropout(l1, p_keep_conv)\n",
    "    with tf.variable_scope('second_conv_max'):\n",
    "        l2a = tf.nn.relu(tf.nn.conv2d(l1, w2,                     # l2a shape=(?, 14, 14, 64)\n",
    "                            strides=[1, 1, 1, 1], padding='SAME'))\n",
    "        l2 = tf.nn.max_pool(l2a, ksize=[1, 2, 2, 1],              # l2 shape=(?, 7, 7, 64)\n",
    "                            strides=[1, 2, 2, 1], padding='SAME')\n",
    "        #l2 = tf.nn.dropout(l2, p_keep_conv)\n",
    "    with tf.variable_scope('third_conv_max'):\n",
    "        l3a = tf.nn.relu(tf.nn.conv2d(l2, w3,                     # l3a shape=(?, 7, 7, 128)\n",
    "                            strides=[1, 1, 1, 1], padding='SAME'))\n",
    "        l3 = tf.nn.max_pool(l3a, ksize=[1, 2, 2, 1],              # l3 shape=(?, 4, 4, 128)\n",
    "                            strides=[1, 2, 2, 1], padding='SAME')\n",
    "    with tf.variable_scope('reshape_fully_connect'):\n",
    "        l3 = tf.reshape(l3, [-1, w4.get_shape().as_list()[0]])    # reshape to (?, 2048)\n",
    "        #l3 = tf.nn.dropout(l3, p_keep_conv)\n",
    "\n",
    "        l4 = tf.nn.relu(tf.matmul(l3, w4))\n",
    "        #l4 = tf.nn.dropout(l4, p_keep_hidden)\n",
    "\n",
    "        pyx = tf.matmul(l4, w_o)\n",
    "    return pyx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "Y = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "w = init_weights([3, 3, 1, 32])       # 3x3x1 conv, 32 outputs\n",
    "w2 = init_weights([3, 3, 32, 64])     # 3x3x32 conv, 64 outputs\n",
    "w3 = init_weights([3, 3, 64, 128])    # 3x3x32 conv, 128 outputs\n",
    "w4 = init_weights([128 * 4 * 4, 625]) # FC 128 * 4 * 4 inputs, 625 outputs\n",
    "w_o = init_weights([625, 10])         # FC 625 inputs, 10 outputs (labels)\n",
    "\n",
    "#p_keep_conv = tf.placeholder(\"float\")\n",
    "#p_keep_hidden = tf.placeholder(\"float\")\n",
    "py_x = model(X, w, w2, w3, w4, w_o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the output and optimizer\n",
    "\n",
    "cost = tf.reduce_mean( tf.square(py_x - Y))\n",
    "train_op = tf.train.GradientDescentOptimizer(1).minimize(cost)\n",
    "predict_op = tf.argmax(py_x, 1)\n",
    "tf.summary.scalar('loss', cost)\n",
    "merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_dir = os.path.join('.', 'log_dir_mnist')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "steps = 0\n",
    "\n",
    "# Launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    # you need to initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    file_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "    \n",
    "    for i in range(100):\n",
    "        train_ind = np.arange(len(trX))\n",
    "        np.random.shuffle(train_ind)\n",
    "        print(train_ind[0:10])\n",
    "        trX = trX[train_ind]\n",
    "        trY = trY[train_ind]\n",
    "        \n",
    "        training_batch = zip(range(0, len(trX), batch_size),\n",
    "                             range(batch_size, len(trX)+1, batch_size))\n",
    "        for start, end in training_batch:\n",
    "            this_summ, this_cost, _ = sess.run([merged_summary,cost,  train_op], \n",
    "                                               feed_dict={X: trX[start:end], Y: trY[start:end]})\n",
    "            \n",
    "            file_writer.add_summary(this_summ, steps)\n",
    "            steps += 1\n",
    "            \n",
    "            if steps%10==0:\n",
    "                print('steps: ', steps, 'loss is: ', this_cost)\n",
    "        test_indices = np.arange(len(teX)) # Get A Test Batch\n",
    "        np.random.shuffle(test_indices)\n",
    "        test_indices = test_indices[0:test_size]\n",
    "        \n",
    "        \n",
    "        print(i, np.mean(np.argmax(teY[test_indices], axis=1) ==\n",
    "                         sess.run(predict_op, feed_dict={X: teX[test_indices],\n",
    "                                                         Y: teY[test_indices]}\n",
    "                                                         )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# go through https://github.com/aymericdamien/TensorFlow-Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB DATASET\n",
    "\n",
    ">Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
    "\n",
    ">As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(25000,) train sequences\n",
      "(25000,) test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "'''Train a recurrent convolutional network on the IMDB sentiment\n",
    "classification task.\n",
    "'''\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "max_features = 10000\n",
    "maxlen = 100  # the max length of each setence.\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train_org, y_train_org), (x_test_org, y_test_org) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "print((x_train_org.shape), 'train sequences')\n",
    "print((x_test_org.shape), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train_org, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test_org, maxlen=maxlen)\n",
    "y_train = y_train_org\n",
    "y_test  = y_test_org\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:',  x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Each line is one sentence.\n",
    "Each number represent one word. The the label contains only one value indicating the positive and negtive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negtive sentence\n",
      "\n",
      "the input sequence is:\n",
      " [   4 1654 1986    7    2   23   27   96    8    6 1304   19 4205    5    4\n",
      "  500   10   10  323 3199 2102   16 1815 3009   34  167  670    2   37   12\n",
      "  272   40   16  115   23    4  270    4   38  446  229    9   43   14  499\n",
      "    7 4767   67   85  857   18    4  111    2  469    4  513    7    2   47\n",
      "    6  171   52  388   21    9 1116 2262   34   78  802    4  277    2  131\n",
      "  225    4    2    7    6   52  206 1154  133    6 2579 1991  310   19   53\n",
      " 8904  206    5 4291 1062  238   60   30  184   52]\n",
      "the corresponding label is:  0\n",
      "\n",
      "Positive sentence\n",
      "\n",
      "the input sequence is:\n",
      " [  78 1099   17 2345    2   21   27 9685 6139    5    2 1603   92 1183    4\n",
      " 1310    7    4  204   42   97   90   35  221  109   29  127   27  118    8\n",
      "   97   12  157   21 6789    2    9    6   66   78 1099    4  631 1191    5\n",
      " 2642  272  191 1070    6 7585    8 2197    2    2  544    5  383 1271  848\n",
      " 1468    2  497    2    8 1597 8778    2   21   60   27  239    9   43 8368\n",
      "  209  405   10   10   12  764   40    4  248   20   12   16    5  174 1791\n",
      "   72    7   51    6 1739   22    4  204  131    9]\n",
      "the corresponding label is:  0\n"
     ]
    }
   ],
   "source": [
    "print('Negtive sentence\\n')\n",
    "print('the input sequence is:\\n', x_train[1000])\n",
    "print('the corresponding label is: ', y_train[1000])\n",
    "\n",
    "print('\\nPositive sentence\\n')\n",
    "print('the input sequence is:\\n', x_train[-1])\n",
    "print('the corresponding label is: ', y_train[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding.\n",
    "Now, each sentence only contains a list of Integer. But during learning, we need \n",
    "## There are two types of word encoding systems\n",
    "1. One-hot encoding, each work is present as [1,0,0,0],... [0,0,0,1]. For the genetic cases, there are ATCG, only for characters. This type of coding usually works just fine.\n",
    "\n",
    "2. For this setiment task, the number of vocabulary can be of size 20,000. So instead of getting a vector of length that long. We can embedding them into a much more compact size, like 128. The embedding matrix in this case, is a $20000 \\times 128$ matrix. And this matrix is a trainable parameter as well. For more information, please refer to [Word embedding](http://sebastianruder.com/word-embeddings-1/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding of the input sentence is of shape:  (?, 100, 128)\n"
     ]
    }
   ],
   "source": [
    "# Embedding\n",
    "\n",
    "embedding_size = 128\n",
    "\n",
    "#inputs = Input(batch_shape=(None, maxlen,))\n",
    "inputs = tf.placeholder(tf.int64, shape = [None, maxlen])\n",
    "out = Embedding(max_features, embedding_size, input_length=maxlen)(inputs)\n",
    "out = Dropout(0.25)(out)\n",
    "print(\"The embedding of the input sentence is of shape: \", out.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution and maxpooling of the input sequence\n",
    "1. 1d convolution can be used to extract features, it can be seen as fused the neighbooring words together to a more abstract representation.\n",
    "2. Max pooling is applied to shrink the sentence length. In the following case, the input sentence is shrinked by 4 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the preprocessed sentences is:  (?, 24, 64)\n"
     ]
    }
   ],
   "source": [
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "shrink_size = 4\n",
    "\n",
    "out = Conv1D(filters,\n",
    "             kernel_size,\n",
    "             padding='valid',\n",
    "             activation='relu',\n",
    "             strides=1)(out)\n",
    "\n",
    "out = MaxPooling1D(shrink_size)(out)\n",
    "\n",
    "print('The size of the preprocessed sentences is: ', out.get_shape())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The RNN cell\n",
    "\n",
    "<!---  ![RNN](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg)<element height=\"50%\" width=\"50%\"> -->\n",
    "\n",
    "## The computation in a rnn cell\n",
    "RNN compute update it's current hidden state based on current inputs(optional) and the hidden state from last time step. The connection that links it's current state and past state is called recurrent connections. \n",
    "\n",
    "### First of all, you need a initia state. \n",
    "$h_0 = \\mathbf{0}$ \n",
    "\n",
    "### For $t$ from 1 to $T$:\n",
    "$h_t = x_tW + h_{t-1}U + b$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished building the recurrent model.\n"
     ]
    }
   ],
   "source": [
    "def rnn_cell(rnn_input, state, input_size, state_size):      \n",
    "    try:\n",
    "        with tf.variable_scope('rnn_cell'):\n",
    "            W = tf.get_variable('W', [input_size, state_size])\n",
    "            U = tf.get_variable('U', [state_size, state_size])\n",
    "            b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    with tf.variable_scope('rnn_cell', reuse=True):\n",
    "        W = tf.get_variable('W', [input_size, state_size])\n",
    "        U = tf.get_variable('U', [state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    return tf.tanh(tf.matmul(rnn_input, W) + tf.matmul(state, U)  + b)\n",
    " \n",
    "def RNN(inputs, init_state, input_size,state_size, time_step):\n",
    "\n",
    "    state = init_state\n",
    "    for idx in range(time_step):\n",
    "        with tf.name_scope('rnn_step_{}'.format(idx)):\n",
    "            state = rnn_cell(inputs[:,idx,:], state, input_size, state_size)\n",
    "    return state\n",
    "\n",
    "\n",
    "state_size = 128  # the size of the hidden state.\n",
    "rnn_input_size = filters \n",
    "time_steps = out.get_shape()[1]\n",
    "\n",
    "init_state = tf.placeholder(tf.float32, [None, state_size])\n",
    "\n",
    "out = RNN(out, init_state, rnn_input_size, state_size, time_steps)\n",
    "\n",
    "print('Finished building the recurrent model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take the output of the RNN\n",
    "1. Map it to a probability indicating the positive or negtive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish constructing the graph.\n"
     ]
    }
   ],
   "source": [
    "out = Dense(1)(out)\n",
    "out = tf.nn.sigmoid(out)\n",
    "\n",
    "target = tf.placeholder(tf.float32, out.get_shape())\n",
    "\n",
    "#losses and train_step\n",
    "element_loss = K.binary_crossentropy(out, target, from_logits=False)\n",
    "#element_loss = tf.square(out- target)\n",
    "\n",
    "total_loss = tf.reduce_mean(element_loss)\n",
    "tf.summary.scalar('loss', total_loss)\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "learning_rate = 1e-1\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "print('finish constructing the graph.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "#sess = tf.InteractiveSession()\n",
    "ckpt_dir = os.path.join('.', 'model')\n",
    "\n",
    "def save_model(sess, saver, checkpoint_dir, model_name, step):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    saver.save(sess, os.path.join(checkpoint_dir, model_name), global_step=step)\n",
    "    \n",
    "def load_model(sess, saver, checkpoint_dir):\n",
    "    print(\"[*] Reading checkpoints...\")\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        saver.restore(sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Network.\n",
    "1. Open a terminal, type the following to monitor the training procedure at http://127.0.1.1:6006/.\n",
    "   ```\n",
    "   tensorboard --logdir='./log_dir'\n",
    "   ```\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.framework.ops.Graph object at 0x7efc2ed49828>\n",
      "[*] Reading checkpoints...\n",
      "Average loss at step 50 for last 100 steps: 1.05286389112\n",
      "Average loss at step 100 for last 100 steps: 0.93197920084\n",
      "Average loss at step 150 for last 100 steps: 0.850394327641\n",
      "Average loss at step 200 for last 100 steps: 0.786729800701\n",
      "Average loss at step 250 for last 100 steps: 0.98144269228\n",
      "Average loss at step 300 for last 100 steps: 0.813057712317\n",
      "Average loss at step 350 for last 100 steps: 0.759086581469\n",
      "Average loss at step 400 for last 100 steps: 0.833176500797\n",
      "Average loss at step 450 for last 100 steps: 0.810520368814\n",
      "Average loss at step 500 for last 100 steps: 0.873464934826\n",
      "Average loss at step 550 for last 100 steps: 1.08853302121\n",
      "Average loss at step 600 for last 100 steps: 0.938685026169\n",
      "Average loss at step 650 for last 100 steps: 0.799125505686\n",
      "Average loss at step 700 for last 100 steps: 0.853672667742\n",
      "Average loss at step 750 for last 100 steps: 0.795592676401\n",
      "Average loss at step 800 for last 100 steps: 0.80948040247\n",
      "Average loss at step 850 for last 100 steps: 0.242856929302\n",
      "Average loss at step 900 for last 100 steps: 0.719115927219\n",
      "Average loss at step 950 for last 100 steps: 1.06592292309\n",
      "Average loss at step 1000 for last 100 steps: 0.746684304476\n",
      "Average loss at step 1050 for last 100 steps: 0.841693487167\n",
      "Average loss at step 1100 for last 100 steps: 0.777627671957\n",
      "Average loss at step 1150 for last 100 steps: 0.76642739892\n",
      "Average loss at step 1200 for last 100 steps: 0.88802905798\n",
      "Average loss at step 1250 for last 100 steps: 0.800381023884\n",
      "Average loss at step 1300 for last 100 steps: 0.813730121851\n",
      "Average loss at step 1350 for last 100 steps: 0.806550393105\n",
      "Average loss at step 1400 for last 100 steps: 0.748580129147\n",
      "Average loss at step 1450 for last 100 steps: 0.806061627865\n",
      "Average loss at step 1500 for last 100 steps: 0.872911874056\n",
      "Average loss at step 1550 for last 100 steps: 0.849320386648\n",
      "Average loss at step 1600 for last 100 steps: 1.06873803496\n",
      "Average loss at step 1650 for last 100 steps: 1.08719462991\n",
      "Average loss at step 1700 for last 100 steps: 0.617660778761\n",
      "Average loss at step 1750 for last 100 steps: 0.778633133173\n",
      "Average loss at step 1800 for last 100 steps: 0.840479016304\n",
      "Average loss at step 1850 for last 100 steps: 0.819707753658\n",
      "Average loss at step 1900 for last 100 steps: 0.769263567924\n",
      "Average loss at step 1950 for last 100 steps: 0.886037963629\n",
      "Average loss at step 2000 for last 100 steps: 1.04994137764\n",
      "Average loss at step 2050 for last 100 steps: 0.872852851152\n",
      "Average loss at step 2100 for last 100 steps: 0.830161818266\n",
      "Average loss at step 2150 for last 100 steps: 0.848943941593\n",
      "Average loss at step 2200 for last 100 steps: 0.851493371725\n",
      "Average loss at step 2250 for last 100 steps: 0.803697344065\n",
      "Average loss at step 2300 for last 100 steps: 0.76987560153\n",
      "Average loss at step 2350 for last 100 steps: 0.760007508993\n",
      "Average loss at step 2400 for last 100 steps: 0.836387639046\n",
      "Average loss at step 2450 for last 100 steps: 0.843222585917\n",
      "Average loss at step 2500 for last 100 steps: 0.921845949888\n",
      "Average loss at step 2550 for last 100 steps: 0.88322670579\n",
      "Average loss at step 2600 for last 100 steps: 0.885463465452\n",
      "Average loss at step 2650 for last 100 steps: 0.770111448765\n",
      "Average loss at step 2700 for last 100 steps: 0.860980213881\n",
      "Average loss at step 2750 for last 100 steps: 0.907386499643\n",
      "Average loss at step 2800 for last 100 steps: 0.925126832724\n",
      "Average loss at step 2850 for last 100 steps: 0.795405036211\n",
      "Average loss at step 2900 for last 100 steps: 0.825970845222\n",
      "Average loss at step 2950 for last 100 steps: 0.804518165588\n",
      "Average loss at step 3000 for last 100 steps: 0.862833256721\n",
      "Average loss at step 3050 for last 100 steps: 0.775447639227\n",
      "Average loss at step 3100 for last 100 steps: 0.770254813433\n",
      "Average loss at step 3150 for last 100 steps: 0.737485226393\n",
      "Average loss at step 3200 for last 100 steps: 0.824372220039\n",
      "Average loss at step 3250 for last 100 steps: 0.786242330074\n",
      "Average loss at step 3300 for last 100 steps: 0.851256740093\n",
      "Average loss at step 3350 for last 100 steps: 0.228719825745\n",
      "Average loss at step 3400 for last 100 steps: 0.804564619064\n",
      "Average loss at step 3450 for last 100 steps: 0.766681778431\n",
      "Average loss at step 3500 for last 100 steps: 0.841101305485\n",
      "Average loss at step 3550 for last 100 steps: 0.747824472189\n",
      "Average loss at step 3600 for last 100 steps: 0.79972053647\n",
      "Average loss at step 3650 for last 100 steps: 0.852003796101\n",
      "Average loss at step 3700 for last 100 steps: 0.839428052902\n",
      "Average loss at step 3750 for last 100 steps: 0.734210019112\n",
      "Average loss at step 3800 for last 100 steps: 0.917107689381\n",
      "Average loss at step 3850 for last 100 steps: 0.956526759863\n",
      "Average loss at step 3900 for last 100 steps: 0.860745222569\n",
      "Average loss at step 3950 for last 100 steps: 0.780338330269\n",
      "Average loss at step 4000 for last 100 steps: 0.807588777542\n",
      "Average loss at step 4050 for last 100 steps: 0.851508482695\n",
      "Average loss at step 4100 for last 100 steps: 0.839072585106\n",
      "Average loss at step 4150 for last 100 steps: 0.805682296753\n",
      "Average loss at step 4200 for last 100 steps: 0.758110432625\n",
      "Average loss at step 4250 for last 100 steps: 0.83727167964\n",
      "Average loss at step 4300 for last 100 steps: 0.770612764359\n",
      "Average loss at step 4350 for last 100 steps: 0.949198750257\n",
      "Average loss at step 4400 for last 100 steps: 1.12275414705\n",
      "Average loss at step 4450 for last 100 steps: 0.822529828548\n",
      "Average loss at step 4500 for last 100 steps: 0.834154419899\n",
      "Average loss at step 4550 for last 100 steps: 1.02861323357\n",
      "Average loss at step 4600 for last 100 steps: 0.766157810688\n",
      "Average loss at step 4650 for last 100 steps: 0.807795387506\n",
      "Average loss at step 4700 for last 100 steps: 0.896621990204\n",
      "Average loss at step 4750 for last 100 steps: 0.747427885532\n",
      "Average loss at step 4800 for last 100 steps: 0.923846988678\n",
      "Average loss at step 4850 for last 100 steps: 0.740281004906\n",
      "Average loss at step 4900 for last 100 steps: 1.0490081358\n",
      "Average loss at step 4950 for last 100 steps: 0.97075756669\n",
      "Average loss at step 5000 for last 100 steps: 0.772539108992\n",
      "Average loss at step 5050 for last 100 steps: 0.717016986609\n",
      "Average loss at step 5100 for last 100 steps: 0.907141603231\n",
      "Average loss at step 5150 for last 100 steps: 0.901920615435\n",
      "Average loss at step 5200 for last 100 steps: 0.835278003216\n",
      "Average loss at step 5250 for last 100 steps: 0.736431229115\n",
      "Average loss at step 5300 for last 100 steps: 0.946828001738\n",
      "Average loss at step 5350 for last 100 steps: 0.767245762348\n",
      "Average loss at step 5400 for last 100 steps: 0.772981777191\n",
      "Average loss at step 5450 for last 100 steps: 0.774513872862\n",
      "Average loss at step 5500 for last 100 steps: 0.792243865728\n",
      "Average loss at step 5550 for last 100 steps: 0.762779808044\n",
      "Average loss at step 5600 for last 100 steps: 0.847347513437\n",
      "Average loss at step 5650 for last 100 steps: 0.834490574598\n",
      "Average loss at step 5700 for last 100 steps: 0.829344184399\n",
      "Average loss at step 5750 for last 100 steps: 0.817192099094\n",
      "Average loss at step 5800 for last 100 steps: 0.843883407116\n",
      "Average loss at step 5850 for last 100 steps: 0.211915726662\n",
      "Average loss at step 5900 for last 100 steps: 0.951439841986\n",
      "Average loss at step 5950 for last 100 steps: 0.912997540236\n",
      "Average loss at step 6000 for last 100 steps: 0.828976441622\n",
      "Average loss at step 6050 for last 100 steps: 0.753883444071\n",
      "Average loss at step 6100 for last 100 steps: 0.740336998701\n",
      "Average loss at step 6150 for last 100 steps: 0.769516851902\n",
      "Average loss at step 6200 for last 100 steps: 1.13020334125\n",
      "Average loss at step 6250 for last 100 steps: 0.840458164215\n",
      "Average loss at step 6300 for last 100 steps: 0.852134587765\n",
      "Average loss at step 6350 for last 100 steps: 0.845945411921\n",
      "Average loss at step 6400 for last 100 steps: 0.840165269375\n",
      "Average loss at step 6450 for last 100 steps: 0.814666143656\n",
      "Average loss at step 6500 for last 100 steps: 0.870469764471\n",
      "Average loss at step 6550 for last 100 steps: 0.832813476324\n",
      "Average loss at step 6600 for last 100 steps: 0.955504289865\n",
      "Average loss at step 6650 for last 100 steps: 0.755049307346\n",
      "Average loss at step 6700 for last 100 steps: 0.423504841328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 6750 for last 100 steps: 0.789415348768\n",
      "Average loss at step 6800 for last 100 steps: 0.933137887716\n",
      "Average loss at step 6850 for last 100 steps: 0.745569602251\n",
      "Average loss at step 6900 for last 100 steps: 0.826289693117\n",
      "Average loss at step 6950 for last 100 steps: 0.886658940315\n",
      "Average loss at step 7000 for last 100 steps: 0.753519817591\n",
      "Average loss at step 7050 for last 100 steps: 0.77233704567\n",
      "Average loss at step 7100 for last 100 steps: 0.827213151455\n",
      "Average loss at step 7150 for last 100 steps: 0.82847802639\n",
      "Average loss at step 7200 for last 100 steps: 0.792974154949\n",
      "Average loss at step 7250 for last 100 steps: 0.82409064889\n",
      "Average loss at step 7300 for last 100 steps: 0.858510450125\n",
      "Average loss at step 7350 for last 100 steps: 0.837886815071\n",
      "Average loss at step 7400 for last 100 steps: 0.808259410858\n",
      "Average loss at step 7450 for last 100 steps: 0.801323120594\n",
      "Average loss at step 7500 for last 100 steps: 0.800808663368\n",
      "Average loss at step 7550 for last 100 steps: 0.915863770247\n",
      "Average loss at step 7600 for last 100 steps: 0.961879585981\n",
      "Average loss at step 7650 for last 100 steps: 0.806043068171\n",
      "Average loss at step 7700 for last 100 steps: 1.21645081758\n",
      "Average loss at step 7750 for last 100 steps: 0.96284606576\n",
      "Average loss at step 7800 for last 100 steps: 0.841443316936\n",
      "Average loss at step 7850 for last 100 steps: 0.870649478436\n",
      "Average loss at step 7900 for last 100 steps: 0.737369897366\n",
      "Average loss at step 7950 for last 100 steps: 0.863130047321\n",
      "Average loss at step 8000 for last 100 steps: 1.10764121413\n",
      "Average loss at step 8050 for last 100 steps: 0.802690725327\n",
      "Average loss at step 8100 for last 100 steps: 0.957096474171\n",
      "Average loss at step 8150 for last 100 steps: 0.862367274761\n",
      "Average loss at step 8200 for last 100 steps: 0.778617271185\n",
      "Average loss at step 8250 for last 100 steps: 0.799981195927\n",
      "Average loss at step 8300 for last 100 steps: 0.96719930172\n",
      "Average loss at step 8350 for last 100 steps: 0.179308228493\n",
      "Average loss at step 8400 for last 100 steps: 0.809996255636\n",
      "Average loss at step 8450 for last 100 steps: 0.779015784264\n",
      "Average loss at step 8500 for last 100 steps: 0.911790105104\n",
      "Average loss at step 8550 for last 100 steps: 0.795925469398\n",
      "Average loss at step 8600 for last 100 steps: 0.858264927864\n",
      "Average loss at step 8650 for last 100 steps: 0.815482320786\n",
      "Average loss at step 8700 for last 100 steps: 0.899916502237\n",
      "Average loss at step 8750 for last 100 steps: 0.836644011736\n",
      "Average loss at step 8800 for last 100 steps: 0.830814367533\n",
      "Average loss at step 8850 for last 100 steps: 0.760185137987\n",
      "Average loss at step 8900 for last 100 steps: 0.790018109083\n",
      "Average loss at step 8950 for last 100 steps: 0.877796316147\n",
      "Average loss at step 9000 for last 100 steps: 0.811761488914\n",
      "Average loss at step 9050 for last 100 steps: 0.765547332764\n",
      "Average loss at step 9100 for last 100 steps: 0.800543199778\n",
      "Average loss at step 9150 for last 100 steps: 0.86950724721\n",
      "Average loss at step 9200 for last 100 steps: 0.393177782297\n",
      "Average loss at step 9250 for last 100 steps: 0.816748046875\n",
      "Average loss at step 9300 for last 100 steps: 0.8187533319\n",
      "Average loss at step 9350 for last 100 steps: 0.803165742159\n",
      "Average loss at step 9400 for last 100 steps: 0.797850871086\n",
      "Average loss at step 9450 for last 100 steps: 0.830817245245\n",
      "Average loss at step 9500 for last 100 steps: 0.832989449501\n",
      "Average loss at step 9550 for last 100 steps: 0.776361298561\n",
      "Average loss at step 9600 for last 100 steps: 0.76283048749\n",
      "Average loss at step 9650 for last 100 steps: 0.897851666212\n",
      "Average loss at step 9700 for last 100 steps: 0.903925958872\n",
      "Average loss at step 9750 for last 100 steps: 1.02861523747\n",
      "Average loss at step 9800 for last 100 steps: 0.891464459896\n",
      "Average loss at step 9850 for last 100 steps: 0.943196673393\n",
      "Average loss at step 9900 for last 100 steps: 1.02622239709\n",
      "Average loss at step 9950 for last 100 steps: 1.1248216486\n",
      "Average loss at step 10000 for last 100 steps: 0.751737048626\n",
      "Average loss at step 10050 for last 100 steps: 0.699707653522\n",
      "Average loss at step 10100 for last 100 steps: 0.748595254421\n",
      "Average loss at step 10150 for last 100 steps: 0.755262160301\n",
      "Average loss at step 10200 for last 100 steps: 0.783902239799\n",
      "Average loss at step 10250 for last 100 steps: 0.777272068262\n",
      "Average loss at step 10300 for last 100 steps: 0.7399883008\n",
      "Average loss at step 10350 for last 100 steps: 0.993352946043\n",
      "Average loss at step 10400 for last 100 steps: 0.807078404427\n",
      "Average loss at step 10450 for last 100 steps: 0.808370691538\n",
      "Average loss at step 10500 for last 100 steps: 0.791223403215\n",
      "Average loss at step 10550 for last 100 steps: 0.894489072561\n",
      "Average loss at step 10600 for last 100 steps: 0.829234672785\n",
      "Average loss at step 10650 for last 100 steps: 1.04217597008\n",
      "Average loss at step 10700 for last 100 steps: 0.966265128851\n",
      "Average loss at step 10750 for last 100 steps: 0.743650926352\n",
      "Average loss at step 10800 for last 100 steps: 0.895113239288\n",
      "Average loss at step 10850 for last 100 steps: 0.134012104273\n",
      "Average loss at step 10900 for last 100 steps: 0.951596287489\n",
      "Average loss at step 10950 for last 100 steps: 0.840993881226\n",
      "Average loss at step 11000 for last 100 steps: 0.824706113338\n",
      "Average loss at step 11050 for last 100 steps: 0.978731033802\n",
      "Average loss at step 11100 for last 100 steps: 0.83026342392\n",
      "Average loss at step 11150 for last 100 steps: 0.780268009901\n",
      "Average loss at step 11200 for last 100 steps: 0.748511668444\n",
      "Average loss at step 11250 for last 100 steps: 0.762564119101\n",
      "Average loss at step 11300 for last 100 steps: 0.834832686186\n",
      "Average loss at step 11350 for last 100 steps: 0.894580487013\n",
      "Average loss at step 11400 for last 100 steps: 0.78172919631\n",
      "Average loss at step 11450 for last 100 steps: 0.884457544088\n",
      "Average loss at step 11500 for last 100 steps: 0.759410405159\n",
      "Average loss at step 11550 for last 100 steps: 0.844603840113\n",
      "Average loss at step 11600 for last 100 steps: 1.11356145501\n",
      "Average loss at step 11650 for last 100 steps: 0.97572988987\n",
      "Average loss at step 11700 for last 100 steps: 0.35602453351\n",
      "Average loss at step 11750 for last 100 steps: 0.799059364796\n",
      "Average loss at step 11800 for last 100 steps: 0.821657607555\n",
      "Average loss at step 11850 for last 100 steps: 0.86410933733\n",
      "Average loss at step 11900 for last 100 steps: 0.814239214659\n",
      "Average loss at step 11950 for last 100 steps: 0.886953905821\n",
      "Average loss at step 12000 for last 100 steps: 0.753134332895\n",
      "Average loss at step 12050 for last 100 steps: 0.994545693398\n",
      "Average loss at step 12100 for last 100 steps: 0.853122960329\n",
      "Average loss at step 12150 for last 100 steps: 0.76834138751\n",
      "Average loss at step 12200 for last 100 steps: 1.00261031985\n",
      "Average loss at step 12250 for last 100 steps: 0.908966369629\n",
      "Average loss at step 12300 for last 100 steps: 0.930630586147\n",
      "Average loss at step 12350 for last 100 steps: 0.845500626564\n",
      "Average loss at step 12400 for last 100 steps: 0.861595660448\n",
      "Average loss at step 12450 for last 100 steps: 0.86929851532\n",
      "Average loss at step 12500 for last 100 steps: 0.89136228323\n",
      "Average loss at step 12550 for last 100 steps: 0.677366101742\n",
      "Average loss at step 12600 for last 100 steps: 0.766186995506\n",
      "Average loss at step 12650 for last 100 steps: 0.749634344578\n",
      "Average loss at step 12700 for last 100 steps: 0.803926123381\n",
      "Average loss at step 12750 for last 100 steps: 0.811698418856\n",
      "Average loss at step 12800 for last 100 steps: 0.945150401592\n",
      "Average loss at step 12850 for last 100 steps: 0.812479861975\n",
      "Average loss at step 12900 for last 100 steps: 0.918680653572\n",
      "Average loss at step 12950 for last 100 steps: 0.830219011307\n",
      "Average loss at step 13000 for last 100 steps: 0.820477195978\n",
      "Average loss at step 13050 for last 100 steps: 0.848673692942\n",
      "Average loss at step 13100 for last 100 steps: 0.894507061243\n",
      "Average loss at step 13150 for last 100 steps: 0.813119336367\n",
      "Average loss at step 13200 for last 100 steps: 0.78191493988\n",
      "Average loss at step 13250 for last 100 steps: 0.901766570807\n",
      "Average loss at step 13300 for last 100 steps: 1.02212715149\n",
      "Average loss at step 13350 for last 100 steps: 0.085748077631\n",
      "Average loss at step 13400 for last 100 steps: 0.839858816862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 13450 for last 100 steps: 0.779603840113\n",
      "Average loss at step 13500 for last 100 steps: 0.814087747335\n",
      "Average loss at step 13550 for last 100 steps: 0.929437366724\n",
      "Average loss at step 13600 for last 100 steps: 0.939226088524\n",
      "Average loss at step 13650 for last 100 steps: 0.757536046505\n",
      "Average loss at step 13700 for last 100 steps: 0.829314798117\n",
      "Average loss at step 13750 for last 100 steps: 0.848580356836\n",
      "Average loss at step 13800 for last 100 steps: 0.842262045145\n",
      "Average loss at step 13850 for last 100 steps: 1.04379331231\n",
      "Average loss at step 13900 for last 100 steps: 1.08498283982\n",
      "Average loss at step 13950 for last 100 steps: 0.754808934927\n",
      "Average loss at step 14000 for last 100 steps: 0.880482811928\n",
      "Average loss at step 14050 for last 100 steps: 0.971935219765\n",
      "Average loss at step 14100 for last 100 steps: 0.762451258898\n",
      "Average loss at step 14150 for last 100 steps: 0.768637167215\n",
      "Average loss at step 14200 for last 100 steps: 0.355932972431\n",
      "Average loss at step 14250 for last 100 steps: 0.909172958136\n",
      "Average loss at step 14300 for last 100 steps: 0.885355131626\n",
      "Average loss at step 14350 for last 100 steps: 0.833206577301\n",
      "Average loss at step 14400 for last 100 steps: 0.792243463993\n",
      "Average loss at step 14450 for last 100 steps: 0.795409606695\n",
      "Average loss at step 14500 for last 100 steps: 0.858582892418\n",
      "Average loss at step 14550 for last 100 steps: 0.822014508247\n",
      "Average loss at step 14600 for last 100 steps: 0.925714861155\n",
      "Average loss at step 14650 for last 100 steps: 0.820735471249\n",
      "Average loss at step 14700 for last 100 steps: 0.910340456963\n",
      "Average loss at step 14750 for last 100 steps: 0.996441044807\n",
      "Average loss at step 14800 for last 100 steps: 0.830988286734\n",
      "Average loss at step 14850 for last 100 steps: 0.807076086998\n",
      "Average loss at step 14900 for last 100 steps: 0.826713805199\n",
      "Average loss at step 14950 for last 100 steps: 0.774451996088\n",
      "Average loss at step 15000 for last 100 steps: 0.80799706459\n",
      "Average loss at step 15050 for last 100 steps: 0.634837100506\n",
      "Average loss at step 15100 for last 100 steps: 0.859528721571\n",
      "Average loss at step 15150 for last 100 steps: 0.80561499238\n",
      "Average loss at step 15200 for last 100 steps: 0.755355417728\n",
      "Average loss at step 15250 for last 100 steps: 0.815711706877\n",
      "Average loss at step 15300 for last 100 steps: 0.814848408699\n",
      "Average loss at step 15350 for last 100 steps: 0.800932201147\n",
      "Average loss at step 15400 for last 100 steps: 0.864564555883\n",
      "Average loss at step 15450 for last 100 steps: 0.989476699829\n",
      "Average loss at step 15500 for last 100 steps: 0.777886457443\n",
      "Average loss at step 15550 for last 100 steps: 0.796609660387\n",
      "Average loss at step 15600 for last 100 steps: 0.855329939127\n",
      "Average loss at step 15650 for last 100 steps: 0.912394574881\n",
      "Average loss at step 15700 for last 100 steps: 0.872327196598\n",
      "Average loss at step 15750 for last 100 steps: 0.73913779974\n",
      "Average loss at step 15800 for last 100 steps: 0.844474306107\n",
      "Average loss at step 15850 for last 100 steps: 0.0706554436684\n",
      "Average loss at step 15900 for last 100 steps: 0.949343934059\n",
      "Average loss at step 15950 for last 100 steps: 0.854106998444\n",
      "Average loss at step 16000 for last 100 steps: 0.825224852562\n",
      "Average loss at step 16050 for last 100 steps: 0.852302362919\n",
      "Average loss at step 16100 for last 100 steps: 0.830436694622\n",
      "Average loss at step 16150 for last 100 steps: 0.79871715188\n",
      "Average loss at step 16200 for last 100 steps: 0.816736894846\n",
      "Average loss at step 16250 for last 100 steps: 0.778113880157\n",
      "Average loss at step 16300 for last 100 steps: 0.932425026894\n",
      "Average loss at step 16350 for last 100 steps: 0.78953689456\n",
      "Average loss at step 16400 for last 100 steps: 0.876814877987\n",
      "Average loss at step 16450 for last 100 steps: 0.879674612284\n",
      "Average loss at step 16500 for last 100 steps: 0.908940628767\n",
      "Average loss at step 16550 for last 100 steps: 0.815315181017\n",
      "Average loss at step 16600 for last 100 steps: 0.85705183506\n",
      "Average loss at step 16650 for last 100 steps: 0.851026411057\n",
      "Average loss at step 16700 for last 100 steps: 0.32705352664\n",
      "Average loss at step 16750 for last 100 steps: 0.865169680119\n",
      "Average loss at step 16800 for last 100 steps: 0.744596987963\n",
      "Average loss at step 16850 for last 100 steps: 0.830347881317\n",
      "Average loss at step 16900 for last 100 steps: 0.829869180918\n",
      "Average loss at step 16950 for last 100 steps: 0.852750278711\n",
      "Average loss at step 17000 for last 100 steps: 0.743152644634\n",
      "Average loss at step 17050 for last 100 steps: 1.23554550529\n",
      "Average loss at step 17100 for last 100 steps: 0.871833537817\n",
      "Average loss at step 17150 for last 100 steps: 0.862307649851\n",
      "Average loss at step 17200 for last 100 steps: 0.784940392971\n",
      "Average loss at step 17250 for last 100 steps: 0.821745219231\n",
      "Average loss at step 17300 for last 100 steps: 0.829063663483\n",
      "Average loss at step 17350 for last 100 steps: 0.806706373692\n",
      "Average loss at step 17400 for last 100 steps: 0.838149031401\n",
      "Average loss at step 17450 for last 100 steps: 0.892170563936\n",
      "Average loss at step 17500 for last 100 steps: 0.803378947973\n",
      "Average loss at step 17550 for last 100 steps: 0.664607949257\n",
      "Average loss at step 17600 for last 100 steps: 0.757366518974\n",
      "Average loss at step 17650 for last 100 steps: 0.838492231369\n",
      "Average loss at step 17700 for last 100 steps: 0.843161222935\n",
      "Average loss at step 17750 for last 100 steps: 0.909833449125\n",
      "Average loss at step 17800 for last 100 steps: 0.795899204016\n",
      "Average loss at step 17850 for last 100 steps: 0.793151060343\n",
      "Average loss at step 17900 for last 100 steps: 0.758073203564\n",
      "Average loss at step 17950 for last 100 steps: 0.760013084412\n",
      "Average loss at step 18000 for last 100 steps: 0.872052667141\n",
      "Average loss at step 18050 for last 100 steps: 0.945597479343\n",
      "Average loss at step 18100 for last 100 steps: 0.93478474021\n",
      "Average loss at step 18150 for last 100 steps: 0.829035513401\n",
      "Average loss at step 18200 for last 100 steps: 0.81015491128\n",
      "Average loss at step 18250 for last 100 steps: 0.806696751118\n",
      "Average loss at step 18300 for last 100 steps: 0.799717322588\n",
      "Average loss at step 18350 for last 100 steps: 0.0322850477695\n",
      "Average loss at step 18400 for last 100 steps: 0.803975926638\n",
      "Average loss at step 18450 for last 100 steps: 0.833751957417\n",
      "Average loss at step 18500 for last 100 steps: 0.785290752649\n",
      "Average loss at step 18550 for last 100 steps: 0.859534546137\n",
      "Average loss at step 18600 for last 100 steps: 0.855834133625\n",
      "Average loss at step 18650 for last 100 steps: 0.864138662815\n",
      "Average loss at step 18700 for last 100 steps: 0.825465295315\n",
      "Average loss at step 18750 for last 100 steps: 0.768258053064\n",
      "Average loss at step 18800 for last 100 steps: 0.873618485928\n",
      "Average loss at step 18850 for last 100 steps: 1.01655825019\n",
      "Average loss at step 18900 for last 100 steps: 0.905964512825\n",
      "Average loss at step 18950 for last 100 steps: 0.909438759089\n",
      "Average loss at step 19000 for last 100 steps: 0.755395498276\n",
      "Average loss at step 19050 for last 100 steps: 0.807391419411\n",
      "Average loss at step 19100 for last 100 steps: 0.770680191517\n",
      "Average loss at step 19150 for last 100 steps: 0.886438171864\n",
      "Average loss at step 19200 for last 100 steps: 0.284503337145\n",
      "Average loss at step 19250 for last 100 steps: 1.11230796814\n",
      "Average loss at step 19300 for last 100 steps: 0.911899169683\n",
      "Average loss at step 19350 for last 100 steps: 0.862483296394\n",
      "Average loss at step 19400 for last 100 steps: 0.774901224375\n",
      "Average loss at step 19450 for last 100 steps: 0.908140690327\n",
      "Average loss at step 19500 for last 100 steps: 0.761043263674\n",
      "Average loss at step 19550 for last 100 steps: 0.781601535082\n",
      "Average loss at step 19600 for last 100 steps: 0.824073010683\n",
      "Average loss at step 19650 for last 100 steps: 0.784301842451\n",
      "Average loss at step 19700 for last 100 steps: 0.77622356534\n",
      "Average loss at step 19750 for last 100 steps: 0.999729450941\n",
      "Average loss at step 19800 for last 100 steps: 0.956583365202\n",
      "Average loss at step 19850 for last 100 steps: 0.861733231544\n",
      "Average loss at step 19900 for last 100 steps: 0.876367793083\n",
      "Average loss at step 19950 for last 100 steps: 0.854753895998\n",
      "Average loss at step 20000 for last 100 steps: 0.876446102858\n",
      "Average loss at step 20050 for last 100 steps: 0.589517198801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 20100 for last 100 steps: 0.785332745314\n",
      "Average loss at step 20150 for last 100 steps: 0.867849030495\n",
      "Average loss at step 20200 for last 100 steps: 0.776429787874\n",
      "Average loss at step 20250 for last 100 steps: 0.828925091028\n",
      "Average loss at step 20300 for last 100 steps: 0.798301700354\n",
      "Average loss at step 20350 for last 100 steps: 0.836534019709\n",
      "Average loss at step 20400 for last 100 steps: 0.786626189947\n",
      "Average loss at step 20450 for last 100 steps: 0.825457669497\n",
      "Average loss at step 20500 for last 100 steps: 0.907950203419\n",
      "Average loss at step 20550 for last 100 steps: 0.769873428345\n",
      "Average loss at step 20600 for last 100 steps: 0.859459750652\n",
      "Average loss at step 20650 for last 100 steps: 0.747369452715\n",
      "Average loss at step 20700 for last 100 steps: 0.758803489208\n",
      "Average loss at step 20750 for last 100 steps: 0.78657122016\n",
      "Average loss at step 20800 for last 100 steps: 1.07036514401\n",
      "Average loss at step 20850 for last 100 steps: 1.01792040825\n",
      "Average loss at step 20900 for last 100 steps: 0.815508215427\n",
      "Average loss at step 20950 for last 100 steps: 0.818917706013\n",
      "Average loss at step 21000 for last 100 steps: 0.816299197674\n",
      "Average loss at step 21050 for last 100 steps: 0.785048521757\n",
      "Average loss at step 21100 for last 100 steps: 0.760918495655\n",
      "Average loss at step 21150 for last 100 steps: 0.785029699802\n",
      "Average loss at step 21200 for last 100 steps: 0.892730208635\n",
      "Average loss at step 21250 for last 100 steps: 0.832451053858\n",
      "Average loss at step 21300 for last 100 steps: 0.806129237413\n",
      "Average loss at step 21350 for last 100 steps: 0.894347507954\n",
      "Average loss at step 21400 for last 100 steps: 0.752029607296\n",
      "Average loss at step 21450 for last 100 steps: 0.845426754951\n",
      "Average loss at step 21500 for last 100 steps: 0.823354971409\n",
      "Average loss at step 21550 for last 100 steps: 1.01753796935\n",
      "Average loss at step 21600 for last 100 steps: 0.760486490726\n",
      "Average loss at step 21650 for last 100 steps: 0.82371825099\n",
      "Average loss at step 21700 for last 100 steps: 0.288081979752\n",
      "Average loss at step 21750 for last 100 steps: 0.785667288303\n",
      "Average loss at step 21800 for last 100 steps: 0.91236463666\n",
      "Average loss at step 21850 for last 100 steps: 0.942731389999\n",
      "Average loss at step 21900 for last 100 steps: 0.784847415686\n",
      "Average loss at step 21950 for last 100 steps: 0.832212563753\n",
      "Average loss at step 22000 for last 100 steps: 0.790798584223\n",
      "Average loss at step 22050 for last 100 steps: 0.874661850929\n",
      "Average loss at step 22100 for last 100 steps: 1.29148585439\n",
      "Average loss at step 22150 for last 100 steps: 0.867651746273\n",
      "Average loss at step 22200 for last 100 steps: 1.0209666574\n",
      "Average loss at step 22250 for last 100 steps: 0.965346728563\n",
      "Average loss at step 22300 for last 100 steps: 0.84351924181\n",
      "Average loss at step 22350 for last 100 steps: 0.893336157799\n",
      "Average loss at step 22400 for last 100 steps: 0.885650911331\n",
      "Average loss at step 22450 for last 100 steps: 0.798109776974\n",
      "Average loss at step 22500 for last 100 steps: 0.777356319427\n",
      "Average loss at step 22550 for last 100 steps: 0.501280064583\n",
      "Average loss at step 22600 for last 100 steps: 0.750868530273\n",
      "Average loss at step 22650 for last 100 steps: 0.83676325798\n",
      "Average loss at step 22700 for last 100 steps: 0.879557635784\n",
      "Average loss at step 22750 for last 100 steps: 0.806878325939\n",
      "Average loss at step 22800 for last 100 steps: 0.833432598114\n",
      "Average loss at step 22850 for last 100 steps: 0.867725003958\n",
      "Average loss at step 22900 for last 100 steps: 0.808814456463\n",
      "Average loss at step 22950 for last 100 steps: 0.878578147888\n",
      "Average loss at step 23000 for last 100 steps: 0.898340936899\n",
      "Average loss at step 23050 for last 100 steps: 0.819417295456\n",
      "Average loss at step 23100 for last 100 steps: 0.822642914057\n",
      "Average loss at step 23150 for last 100 steps: 0.789665052891\n",
      "Average loss at step 23200 for last 100 steps: 0.908598151207\n",
      "Average loss at step 23250 for last 100 steps: 0.80491992116\n",
      "Average loss at step 23300 for last 100 steps: 0.967082445621\n",
      "Average loss at step 23350 for last 100 steps: 0.9132934165\n",
      "Average loss at step 23400 for last 100 steps: 0.732816019058\n",
      "Average loss at step 23450 for last 100 steps: 0.770778417587\n",
      "Average loss at step 23500 for last 100 steps: 0.934062255621\n",
      "Average loss at step 23550 for last 100 steps: 0.846448546648\n",
      "Average loss at step 23600 for last 100 steps: 0.939487234354\n",
      "Average loss at step 23650 for last 100 steps: 0.85133505702\n",
      "Average loss at step 23700 for last 100 steps: 0.945379390717\n",
      "Average loss at step 23750 for last 100 steps: 0.811882185936\n",
      "Average loss at step 23800 for last 100 steps: 0.810496206284\n",
      "Average loss at step 23850 for last 100 steps: 0.708096569777\n",
      "Average loss at step 23900 for last 100 steps: 0.785286340714\n",
      "Average loss at step 23950 for last 100 steps: 0.790474618673\n",
      "Average loss at step 24000 for last 100 steps: 0.757776069641\n",
      "Average loss at step 24050 for last 100 steps: 0.77704220891\n",
      "Average loss at step 24100 for last 100 steps: 0.973176652193\n",
      "Average loss at step 24150 for last 100 steps: 0.80277182579\n",
      "Average loss at step 24200 for last 100 steps: 0.488573428392\n",
      "Average loss at step 24250 for last 100 steps: 0.96999142766\n",
      "Average loss at step 24300 for last 100 steps: 0.794184172153\n",
      "Average loss at step 24350 for last 100 steps: 0.86115606904\n",
      "Average loss at step 24400 for last 100 steps: 0.796317942142\n",
      "Average loss at step 24450 for last 100 steps: 0.821875686646\n",
      "Average loss at step 24500 for last 100 steps: 0.819979665279\n",
      "Average loss at step 24550 for last 100 steps: 0.912577559948\n",
      "Average loss at step 24600 for last 100 steps: 0.860948939323\n",
      "Average loss at step 24650 for last 100 steps: 0.889042716026\n",
      "Average loss at step 24700 for last 100 steps: 0.838864020109\n",
      "Average loss at step 24750 for last 100 steps: 0.789329966307\n",
      "Average loss at step 24800 for last 100 steps: 0.909737582207\n",
      "Average loss at step 24850 for last 100 steps: 0.778434070349\n",
      "Average loss at step 24900 for last 100 steps: 0.84406257987\n",
      "Average loss at step 24950 for last 100 steps: 0.849027892351\n",
      "Average loss at step 25000 for last 100 steps: 0.83664396286\n",
      "Average loss at step 25050 for last 100 steps: 0.522658416033\n",
      "Average loss at step 25100 for last 100 steps: 1.06515085101\n",
      "Average loss at step 25150 for last 100 steps: 0.772059020996\n",
      "Average loss at step 25200 for last 100 steps: 0.83591684103\n",
      "Average loss at step 25250 for last 100 steps: 0.862208076715\n",
      "Average loss at step 25300 for last 100 steps: 0.766272624731\n",
      "Average loss at step 25350 for last 100 steps: 0.793598173857\n",
      "Average loss at step 25400 for last 100 steps: 0.900227096081\n",
      "Average loss at step 25450 for last 100 steps: 0.963340178728\n",
      "Average loss at step 25500 for last 100 steps: 0.911974031925\n",
      "Average loss at step 25550 for last 100 steps: 0.955605640411\n",
      "Average loss at step 25600 for last 100 steps: 0.85180175066\n",
      "Average loss at step 25650 for last 100 steps: 0.852719386816\n",
      "Average loss at step 25700 for last 100 steps: 0.988434405327\n",
      "Average loss at step 25750 for last 100 steps: 0.812209671736\n",
      "Average loss at step 25800 for last 100 steps: 0.945666062832\n",
      "Average loss at step 25850 for last 100 steps: 0.762173882723\n",
      "Average loss at step 25900 for last 100 steps: 0.693739237785\n",
      "Average loss at step 25950 for last 100 steps: 0.746994558573\n",
      "Average loss at step 26000 for last 100 steps: 0.861254998446\n",
      "Average loss at step 26050 for last 100 steps: 0.779817942381\n",
      "Average loss at step 26100 for last 100 steps: 0.844163165092\n",
      "Average loss at step 26150 for last 100 steps: 0.895255832672\n",
      "Average loss at step 26200 for last 100 steps: 0.81887537241\n",
      "Average loss at step 26250 for last 100 steps: 0.838223444223\n",
      "Average loss at step 26300 for last 100 steps: 0.863519191742\n",
      "Average loss at step 26350 for last 100 steps: 0.980970387459\n",
      "Average loss at step 26400 for last 100 steps: 0.812751209736\n",
      "Average loss at step 26450 for last 100 steps: 0.756837980747\n",
      "Average loss at step 26500 for last 100 steps: 0.79783882618\n",
      "Average loss at step 26550 for last 100 steps: 0.936955943108\n",
      "Average loss at step 26600 for last 100 steps: 0.863879146576\n",
      "Average loss at step 26650 for last 100 steps: 1.18271722913\n",
      "Average loss at step 26700 for last 100 steps: 0.191286121607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 26750 for last 100 steps: 0.776011167765\n",
      "Average loss at step 26800 for last 100 steps: 0.904799002409\n",
      "Average loss at step 26850 for last 100 steps: 0.960778089762\n",
      "Average loss at step 26900 for last 100 steps: 0.781282446384\n",
      "Average loss at step 26950 for last 100 steps: 0.764607845545\n",
      "Average loss at step 27000 for last 100 steps: 0.845168157816\n",
      "Average loss at step 27050 for last 100 steps: 0.866204053164\n",
      "Average loss at step 27100 for last 100 steps: 0.743490610123\n",
      "Average loss at step 27150 for last 100 steps: 0.839406981468\n",
      "Average loss at step 27200 for last 100 steps: 0.789254211187\n",
      "Average loss at step 27250 for last 100 steps: 0.840509073734\n",
      "Average loss at step 27300 for last 100 steps: 0.748020591736\n",
      "Average loss at step 27350 for last 100 steps: 0.867585482597\n",
      "Average loss at step 27400 for last 100 steps: 0.806713947058\n",
      "Average loss at step 27450 for last 100 steps: 0.748852669001\n",
      "Average loss at step 27500 for last 100 steps: 0.764812122583\n",
      "Average loss at step 27550 for last 100 steps: 0.636898143291\n",
      "Average loss at step 27600 for last 100 steps: 1.06034046412\n",
      "Average loss at step 27650 for last 100 steps: 0.88776453495\n",
      "Average loss at step 27700 for last 100 steps: 0.802153255939\n",
      "Average loss at step 27750 for last 100 steps: 0.868808729649\n",
      "Average loss at step 27800 for last 100 steps: 0.933661750555\n",
      "Average loss at step 27850 for last 100 steps: 0.974460504055\n",
      "Average loss at step 27900 for last 100 steps: 0.790008865595\n",
      "Average loss at step 27950 for last 100 steps: 0.862564016581\n",
      "Average loss at step 28000 for last 100 steps: 1.04914148092\n",
      "Average loss at step 28050 for last 100 steps: 0.89874579072\n",
      "Average loss at step 28100 for last 100 steps: 0.812862844467\n",
      "Average loss at step 28150 for last 100 steps: 0.789842281342\n",
      "Average loss at step 28200 for last 100 steps: 0.824853217602\n",
      "Average loss at step 28250 for last 100 steps: 0.79025965333\n",
      "Average loss at step 28300 for last 100 steps: 0.825251220465\n",
      "Average loss at step 28350 for last 100 steps: 0.804197083712\n",
      "Average loss at step 28400 for last 100 steps: 0.784145795107\n",
      "Average loss at step 28450 for last 100 steps: 0.780765762329\n",
      "Average loss at step 28500 for last 100 steps: 0.941906625032\n",
      "Average loss at step 28550 for last 100 steps: 0.948031843901\n",
      "Average loss at step 28600 for last 100 steps: 1.06108633637\n",
      "Average loss at step 28650 for last 100 steps: 0.812959375381\n",
      "Average loss at step 28700 for last 100 steps: 0.798256230354\n",
      "Average loss at step 28750 for last 100 steps: 0.771702145338\n",
      "Average loss at step 28800 for last 100 steps: 0.758686329126\n",
      "Average loss at step 28850 for last 100 steps: 0.739806587696\n",
      "Average loss at step 28900 for last 100 steps: 0.946115362644\n",
      "Average loss at step 28950 for last 100 steps: 0.846355065107\n",
      "Average loss at step 29000 for last 100 steps: 0.903737767935\n",
      "Average loss at step 29050 for last 100 steps: 0.900196820498\n",
      "Average loss at step 29100 for last 100 steps: 0.799367877245\n",
      "Average loss at step 29150 for last 100 steps: 0.774950945377\n",
      "Average loss at step 29200 for last 100 steps: 0.162090983391\n",
      "Average loss at step 29250 for last 100 steps: 0.794590256214\n",
      "Average loss at step 29300 for last 100 steps: 0.797387769222\n",
      "Average loss at step 29350 for last 100 steps: 0.788103946447\n",
      "Average loss at step 29400 for last 100 steps: 0.833559904099\n",
      "Average loss at step 29450 for last 100 steps: 0.883291374445\n",
      "Average loss at step 29500 for last 100 steps: 0.749933693409\n",
      "Average loss at step 29550 for last 100 steps: 0.80539242506\n",
      "Average loss at step 29600 for last 100 steps: 0.820794992447\n",
      "Average loss at step 29650 for last 100 steps: 0.825324542522\n",
      "Average loss at step 29700 for last 100 steps: 0.799344421625\n",
      "Average loss at step 29750 for last 100 steps: 1.06213772535\n",
      "Average loss at step 29800 for last 100 steps: 0.870190937519\n",
      "Average loss at step 29850 for last 100 steps: 0.799952068329\n",
      "Average loss at step 29900 for last 100 steps: 0.816362122297\n",
      "Average loss at step 29950 for last 100 steps: 0.931052131653\n",
      "Average loss at step 30000 for last 100 steps: 0.881370370388\n",
      "Average loss at step 30050 for last 100 steps: 0.405230139494\n",
      "Average loss at step 30100 for last 100 steps: 0.783320205212\n",
      "Average loss at step 30150 for last 100 steps: 0.852360969782\n",
      "Average loss at step 30200 for last 100 steps: 0.883833037615\n",
      "Average loss at step 30250 for last 100 steps: 0.748566296101\n",
      "Average loss at step 30300 for last 100 steps: 0.904153785706\n",
      "Average loss at step 30350 for last 100 steps: 0.95240688324\n",
      "Average loss at step 30400 for last 100 steps: 0.754944907427\n",
      "Average loss at step 30450 for last 100 steps: 1.07673978686\n",
      "Average loss at step 30500 for last 100 steps: 0.811091009378\n",
      "Average loss at step 30550 for last 100 steps: 0.865102965832\n",
      "Average loss at step 30600 for last 100 steps: 0.775721496344\n",
      "Average loss at step 30650 for last 100 steps: 0.777932367325\n",
      "Average loss at step 30700 for last 100 steps: 0.73767367363\n",
      "Average loss at step 30750 for last 100 steps: 0.879117075205\n",
      "Average loss at step 30800 for last 100 steps: 0.837190822363\n",
      "Average loss at step 30850 for last 100 steps: 0.825966652632\n",
      "Average loss at step 30900 for last 100 steps: 0.702935285568\n",
      "Average loss at step 30950 for last 100 steps: 0.825386320353\n",
      "Average loss at step 31000 for last 100 steps: 0.907950364351\n",
      "Average loss at step 31050 for last 100 steps: 0.960829914808\n",
      "Average loss at step 31100 for last 100 steps: 0.948021787405\n",
      "Average loss at step 31150 for last 100 steps: 0.826317535639\n",
      "Average loss at step 31200 for last 100 steps: 0.820109226704\n",
      "Average loss at step 31250 for last 100 steps: 0.807301061153\n",
      "Average loss at step 31300 for last 100 steps: 0.803568940163\n",
      "Average loss at step 31350 for last 100 steps: 0.842945731878\n",
      "Average loss at step 31400 for last 100 steps: 1.1535639298\n",
      "Average loss at step 31450 for last 100 steps: 0.898133225441\n",
      "Average loss at step 31500 for last 100 steps: 0.817069126368\n",
      "Average loss at step 31550 for last 100 steps: 0.816146605015\n",
      "Average loss at step 31600 for last 100 steps: 0.762817394733\n",
      "Average loss at step 31650 for last 100 steps: 0.848823158741\n",
      "Average loss at step 31700 for last 100 steps: 0.143661381006\n",
      "Average loss at step 31750 for last 100 steps: 0.839498413801\n",
      "Average loss at step 31800 for last 100 steps: 0.948607513905\n",
      "Average loss at step 31850 for last 100 steps: 0.89351981163\n",
      "Average loss at step 31900 for last 100 steps: 0.8326256001\n",
      "Average loss at step 31950 for last 100 steps: 0.851757028103\n",
      "Average loss at step 32000 for last 100 steps: 0.850744926929\n",
      "Average loss at step 32050 for last 100 steps: 0.876459217072\n",
      "Average loss at step 32100 for last 100 steps: 0.743450388908\n",
      "Average loss at step 32150 for last 100 steps: 0.77146422863\n",
      "Average loss at step 32200 for last 100 steps: 0.928309242725\n",
      "Average loss at step 32250 for last 100 steps: 0.796337307692\n",
      "Average loss at step 32300 for last 100 steps: 0.840868319273\n",
      "Average loss at step 32350 for last 100 steps: 0.856183819771\n",
      "Average loss at step 32400 for last 100 steps: 0.863707163334\n",
      "Average loss at step 32450 for last 100 steps: 0.810753238201\n",
      "Average loss at step 32500 for last 100 steps: 0.864581593275\n",
      "Average loss at step 32550 for last 100 steps: 0.451170436144\n",
      "Average loss at step 32600 for last 100 steps: 0.923872299194\n",
      "Average loss at step 32650 for last 100 steps: 0.85461633563\n",
      "Average loss at step 32700 for last 100 steps: 0.786463854313\n",
      "Average loss at step 32750 for last 100 steps: 0.806583704948\n",
      "Average loss at step 32800 for last 100 steps: 0.814444496632\n",
      "Average loss at step 32850 for last 100 steps: 0.814237254858\n",
      "Average loss at step 32900 for last 100 steps: 0.993073880672\n",
      "Average loss at step 32950 for last 100 steps: 0.845978600979\n",
      "Average loss at step 33000 for last 100 steps: 0.935720412731\n",
      "Average loss at step 33050 for last 100 steps: 0.762159869671\n",
      "Average loss at step 33100 for last 100 steps: 0.782559455633\n",
      "Average loss at step 33150 for last 100 steps: 0.836849626303\n",
      "Average loss at step 33200 for last 100 steps: 0.889446228743\n",
      "Average loss at step 33250 for last 100 steps: 0.803257091045\n",
      "Average loss at step 33300 for last 100 steps: 0.806469974518\n",
      "Average loss at step 33350 for last 100 steps: 0.775889081955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 33400 for last 100 steps: 0.752719846964\n",
      "Average loss at step 33450 for last 100 steps: 0.819555077553\n",
      "Average loss at step 33500 for last 100 steps: 0.974437129498\n",
      "Average loss at step 33550 for last 100 steps: 0.83436013341\n",
      "Average loss at step 33600 for last 100 steps: 0.860150581598\n",
      "Average loss at step 33650 for last 100 steps: 0.787514592409\n",
      "Average loss at step 33700 for last 100 steps: 0.796748427153\n",
      "Average loss at step 33750 for last 100 steps: 0.772718272209\n",
      "Average loss at step 33800 for last 100 steps: 0.778722410202\n",
      "Average loss at step 33850 for last 100 steps: 0.776175426245\n",
      "Average loss at step 33900 for last 100 steps: 0.853362455368\n",
      "Average loss at step 33950 for last 100 steps: 0.969118442535\n",
      "Average loss at step 34000 for last 100 steps: 0.78688749671\n",
      "Average loss at step 34050 for last 100 steps: 0.842376995087\n",
      "Average loss at step 34100 for last 100 steps: 0.764127688408\n",
      "Average loss at step 34150 for last 100 steps: 0.820917266607\n",
      "Average loss at step 34200 for last 100 steps: 0.0884812021255\n",
      "Average loss at step 34250 for last 100 steps: 0.851638829708\n",
      "Average loss at step 34300 for last 100 steps: 0.870314897299\n",
      "Average loss at step 34350 for last 100 steps: 0.802793946266\n",
      "Average loss at step 34400 for last 100 steps: 0.871682403088\n",
      "Average loss at step 34450 for last 100 steps: 0.78229904294\n",
      "Average loss at step 34500 for last 100 steps: 0.814393104315\n",
      "Average loss at step 34550 for last 100 steps: 0.819496906996\n",
      "Average loss at step 34600 for last 100 steps: 0.91587553978\n",
      "Average loss at step 34650 for last 100 steps: 1.0023434782\n",
      "Average loss at step 34700 for last 100 steps: 0.931076475382\n",
      "Average loss at step 34750 for last 100 steps: 0.769953616858\n",
      "Average loss at step 34800 for last 100 steps: 0.750189139843\n",
      "Average loss at step 34850 for last 100 steps: 0.780348900557\n",
      "Average loss at step 34900 for last 100 steps: 0.83137460947\n",
      "Average loss at step 34950 for last 100 steps: 0.901556286812\n",
      "Average loss at step 35000 for last 100 steps: 0.818621041775\n",
      "Average loss at step 35050 for last 100 steps: 0.388094648123\n",
      "Average loss at step 35100 for last 100 steps: 0.860528713465\n",
      "Average loss at step 35150 for last 100 steps: 0.795861996412\n",
      "Average loss at step 35200 for last 100 steps: 0.888193010092\n",
      "Average loss at step 35250 for last 100 steps: 0.84132871151\n",
      "Average loss at step 35300 for last 100 steps: 0.813743499517\n",
      "Average loss at step 35350 for last 100 steps: 0.892153916359\n",
      "Average loss at step 35400 for last 100 steps: 1.14207065225\n",
      "Average loss at step 35450 for last 100 steps: 1.21267150879\n",
      "Average loss at step 35500 for last 100 steps: 1.03924049973\n",
      "Average loss at step 35550 for last 100 steps: 0.774531383514\n",
      "Average loss at step 35600 for last 100 steps: 0.779370690584\n",
      "Average loss at step 35650 for last 100 steps: 0.843385778666\n",
      "Average loss at step 35700 for last 100 steps: 0.823167999983\n",
      "Average loss at step 35750 for last 100 steps: 0.823173820972\n",
      "Average loss at step 35800 for last 100 steps: 0.794293364286\n",
      "Average loss at step 35850 for last 100 steps: 0.886833567619\n",
      "Average loss at step 35900 for last 100 steps: 0.687813458443\n",
      "Average loss at step 35950 for last 100 steps: 0.830263770819\n",
      "Average loss at step 36000 for last 100 steps: 0.84880564332\n",
      "Average loss at step 36050 for last 100 steps: 0.907876318693\n",
      "Average loss at step 36100 for last 100 steps: 0.882274816036\n",
      "Average loss at step 36150 for last 100 steps: 0.766567653418\n",
      "Average loss at step 36200 for last 100 steps: 0.800169727802\n",
      "Average loss at step 36250 for last 100 steps: 0.840570346117\n",
      "Average loss at step 36300 for last 100 steps: 0.772205075026\n",
      "Average loss at step 36350 for last 100 steps: 0.7769196105\n",
      "Average loss at step 36400 for last 100 steps: 0.922585808039\n",
      "Average loss at step 36450 for last 100 steps: 0.879112420082\n",
      "Average loss at step 36500 for last 100 steps: 0.745618935823\n",
      "Average loss at step 36550 for last 100 steps: 0.901533761024\n",
      "Average loss at step 36600 for last 100 steps: 0.855153391361\n",
      "Average loss at step 36650 for last 100 steps: 0.896358728409\n",
      "Average loss at step 36700 for last 100 steps: 0.0606631541252\n",
      "Average loss at step 36750 for last 100 steps: 0.793400678635\n",
      "Average loss at step 36800 for last 100 steps: 0.870436179638\n",
      "Average loss at step 36850 for last 100 steps: 0.794028302431\n",
      "Average loss at step 36900 for last 100 steps: 0.923084890842\n",
      "Average loss at step 36950 for last 100 steps: 0.771351866722\n",
      "Average loss at step 37000 for last 100 steps: 0.885165994167\n",
      "Average loss at step 37050 for last 100 steps: 0.797453035116\n",
      "Average loss at step 37100 for last 100 steps: 0.798103685379\n",
      "Average loss at step 37150 for last 100 steps: 0.749952163696\n",
      "Average loss at step 37200 for last 100 steps: 0.802885252237\n",
      "Average loss at step 37250 for last 100 steps: 0.747197641134\n",
      "Average loss at step 37300 for last 100 steps: 1.03718208075\n",
      "Average loss at step 37350 for last 100 steps: 0.791352643967\n",
      "Average loss at step 37400 for last 100 steps: 0.800210107565\n",
      "Average loss at step 37450 for last 100 steps: 0.947775256634\n",
      "Average loss at step 37500 for last 100 steps: 0.817211495638\n",
      "Average loss at step 37550 for last 100 steps: 0.341627624035\n",
      "Average loss at step 37600 for last 100 steps: 0.819611468315\n",
      "Average loss at step 37650 for last 100 steps: 0.871986548901\n",
      "Average loss at step 37700 for last 100 steps: 0.791192724705\n",
      "Average loss at step 37750 for last 100 steps: 0.779384202957\n",
      "Average loss at step 37800 for last 100 steps: 0.775168559551\n",
      "Average loss at step 37850 for last 100 steps: 0.809189798832\n",
      "Average loss at step 37900 for last 100 steps: 1.00500319004\n",
      "Average loss at step 37950 for last 100 steps: 0.78836345315\n",
      "Average loss at step 38000 for last 100 steps: 0.901708009243\n",
      "Average loss at step 38050 for last 100 steps: 0.855306614637\n",
      "Average loss at step 38100 for last 100 steps: 0.862934201956\n",
      "Average loss at step 38150 for last 100 steps: 0.764950302839\n",
      "Average loss at step 38200 for last 100 steps: 0.852358176708\n",
      "Average loss at step 38250 for last 100 steps: 0.840281352997\n",
      "Average loss at step 38300 for last 100 steps: 0.986453583241\n",
      "Average loss at step 38350 for last 100 steps: 0.879476608038\n",
      "Average loss at step 38400 for last 100 steps: 0.622099188566\n",
      "Average loss at step 38450 for last 100 steps: 0.792115470171\n",
      "Average loss at step 38500 for last 100 steps: 0.972834275961\n",
      "Average loss at step 38550 for last 100 steps: 0.849467228651\n",
      "Average loss at step 38600 for last 100 steps: 1.01594547629\n",
      "Average loss at step 38650 for last 100 steps: 0.865080116987\n",
      "Average loss at step 38700 for last 100 steps: 0.818790642023\n",
      "Average loss at step 38750 for last 100 steps: 0.902645872831\n",
      "Average loss at step 38800 for last 100 steps: 0.829791580439\n",
      "Average loss at step 38850 for last 100 steps: 0.772430632114\n",
      "Average loss at step 38900 for last 100 steps: 0.756476931572\n",
      "Average loss at step 38950 for last 100 steps: 0.838091864586\n",
      "Average loss at step 39000 for last 100 steps: 0.898861914873\n",
      "Average loss at step 39050 for last 100 steps: 0.947394615412\n",
      "Average loss at step 39100 for last 100 steps: 1.35411139011\n",
      "Average loss at step 39150 for last 100 steps: 0.822041085958\n",
      "Average loss at step 39200 for last 100 steps: 0.029383379221\n",
      "Average loss at step 39250 for last 100 steps: 0.778154071569\n",
      "Average loss at step 39300 for last 100 steps: 0.773138252497\n",
      "Average loss at step 39350 for last 100 steps: 0.889270319939\n",
      "Average loss at step 39400 for last 100 steps: 0.822442039251\n",
      "Average loss at step 39450 for last 100 steps: 0.865106841326\n",
      "Average loss at step 39500 for last 100 steps: 0.792236281633\n",
      "Average loss at step 39550 for last 100 steps: 0.789991089106\n",
      "Average loss at step 39600 for last 100 steps: 0.901092313528\n",
      "Average loss at step 39650 for last 100 steps: 0.809328774214\n",
      "Average loss at step 39700 for last 100 steps: 1.31465660691\n",
      "Average loss at step 39750 for last 100 steps: 0.891657563448\n",
      "Average loss at step 39800 for last 100 steps: 0.718491075039\n",
      "Average loss at step 39850 for last 100 steps: 0.99963252306\n",
      "Average loss at step 39900 for last 100 steps: 0.862845317125\n",
      "Average loss at step 39950 for last 100 steps: 1.01575475931\n",
      "Average loss at step 40000 for last 100 steps: 0.789386926889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 40050 for last 100 steps: 0.39782556057\n",
      "Average loss at step 40100 for last 100 steps: 0.918773202896\n",
      "Average loss at step 40150 for last 100 steps: 0.789845912457\n",
      "Average loss at step 40200 for last 100 steps: 0.823393565416\n",
      "Average loss at step 40250 for last 100 steps: 0.935655248165\n",
      "Average loss at step 40300 for last 100 steps: 0.96062812686\n",
      "Average loss at step 40350 for last 100 steps: 0.722795089483\n",
      "Average loss at step 40400 for last 100 steps: 0.85371468544\n",
      "Average loss at step 40450 for last 100 steps: 0.787440966368\n",
      "Average loss at step 40500 for last 100 steps: 0.858452695608\n",
      "Average loss at step 40550 for last 100 steps: 0.961546908617\n",
      "Average loss at step 40600 for last 100 steps: 0.836574673653\n",
      "Average loss at step 40650 for last 100 steps: 0.782637857199\n",
      "Average loss at step 40700 for last 100 steps: 0.760778000355\n",
      "Average loss at step 40750 for last 100 steps: 0.77100443244\n",
      "Average loss at step 40800 for last 100 steps: 0.83914313674\n",
      "Average loss at step 40850 for last 100 steps: 0.862256872654\n",
      "Average loss at step 40900 for last 100 steps: 0.613039065599\n",
      "Average loss at step 40950 for last 100 steps: 0.753450568914\n",
      "Average loss at step 41000 for last 100 steps: 0.809607855082\n",
      "Average loss at step 41050 for last 100 steps: 1.06371925473\n",
      "Average loss at step 41100 for last 100 steps: 0.803538286686\n",
      "Average loss at step 41150 for last 100 steps: 0.777653176785\n",
      "Average loss at step 41200 for last 100 steps: 0.904584904909\n",
      "Average loss at step 41250 for last 100 steps: 0.80952870965\n",
      "Average loss at step 41300 for last 100 steps: 0.893192681074\n",
      "Average loss at step 41350 for last 100 steps: 0.828217953444\n",
      "Average loss at step 41400 for last 100 steps: 0.946735072136\n",
      "Average loss at step 41450 for last 100 steps: 0.895874181986\n",
      "Average loss at step 41500 for last 100 steps: 0.82935005188\n",
      "Average loss at step 41550 for last 100 steps: 0.742162536383\n",
      "Average loss at step 41600 for last 100 steps: 0.79327835083\n",
      "Average loss at step 41650 for last 100 steps: 0.748781987429\n",
      "Average loss at step 41700 for last 100 steps: 0.820172070265\n",
      "Average loss at step 41750 for last 100 steps: 0.83905436039\n",
      "Average loss at step 41800 for last 100 steps: 0.790680897236\n",
      "Average loss at step 41850 for last 100 steps: 0.778387557268\n",
      "Average loss at step 41900 for last 100 steps: 0.759447348118\n",
      "Average loss at step 41950 for last 100 steps: 0.84037894249\n",
      "Average loss at step 42000 for last 100 steps: 0.836280115843\n",
      "Average loss at step 42050 for last 100 steps: 0.84582742691\n",
      "Average loss at step 42100 for last 100 steps: 0.801676472425\n",
      "Average loss at step 42150 for last 100 steps: 0.948725713491\n",
      "Average loss at step 42200 for last 100 steps: 0.895622454882\n",
      "Average loss at step 42250 for last 100 steps: 0.876814143658\n",
      "Average loss at step 42300 for last 100 steps: 1.04713364482\n",
      "Average loss at step 42350 for last 100 steps: 0.924292156696\n",
      "Average loss at step 42400 for last 100 steps: 0.910260653496\n",
      "Average loss at step 42450 for last 100 steps: 0.821215533018\n",
      "Average loss at step 42500 for last 100 steps: 0.791846876144\n",
      "Average loss at step 42550 for last 100 steps: 0.239294632673\n",
      "Average loss at step 42600 for last 100 steps: 0.831815625429\n",
      "Average loss at step 42650 for last 100 steps: 0.786609218121\n",
      "Average loss at step 42700 for last 100 steps: 0.781282730103\n",
      "Average loss at step 42750 for last 100 steps: 0.724440008402\n",
      "Average loss at step 42800 for last 100 steps: 0.826392066479\n",
      "Average loss at step 42850 for last 100 steps: 0.840954784155\n",
      "Average loss at step 42900 for last 100 steps: 0.812530925274\n",
      "Average loss at step 42950 for last 100 steps: 0.819481585026\n",
      "Average loss at step 43000 for last 100 steps: 0.897830502987\n",
      "Average loss at step 43050 for last 100 steps: 0.799675989151\n",
      "Average loss at step 43100 for last 100 steps: 0.855169131756\n",
      "Average loss at step 43150 for last 100 steps: 0.809376645088\n",
      "Average loss at step 43200 for last 100 steps: 1.01119928598\n",
      "Average loss at step 43250 for last 100 steps: 0.777086975574\n",
      "Average loss at step 43300 for last 100 steps: 0.90875516057\n",
      "Average loss at step 43350 for last 100 steps: 0.968801096678\n",
      "Average loss at step 43400 for last 100 steps: 0.554116822481\n",
      "Average loss at step 43450 for last 100 steps: 0.866088793278\n",
      "Average loss at step 43500 for last 100 steps: 0.771171591282\n",
      "Average loss at step 43550 for last 100 steps: 0.893452193737\n",
      "Average loss at step 43600 for last 100 steps: 0.760695078373\n",
      "Average loss at step 43650 for last 100 steps: 0.804244639874\n",
      "Average loss at step 43700 for last 100 steps: 1.03721260309\n",
      "Average loss at step 43750 for last 100 steps: 1.05839972854\n",
      "Average loss at step 43800 for last 100 steps: 0.725914362669\n",
      "Average loss at step 43850 for last 100 steps: 0.789725456238\n",
      "Average loss at step 43900 for last 100 steps: 0.771401128769\n",
      "Average loss at step 43950 for last 100 steps: 0.768600707054\n",
      "Average loss at step 44000 for last 100 steps: 0.776173784733\n",
      "Average loss at step 44050 for last 100 steps: 0.791338305473\n",
      "Average loss at step 44100 for last 100 steps: 0.918015859127\n",
      "Average loss at step 44150 for last 100 steps: 0.785077819824\n",
      "Average loss at step 44200 for last 100 steps: 1.00401280999\n",
      "Average loss at step 44250 for last 100 steps: 0.844148015976\n",
      "Average loss at step 44300 for last 100 steps: 1.11326194525\n",
      "Average loss at step 44350 for last 100 steps: 0.809758105278\n",
      "Average loss at step 44400 for last 100 steps: 0.800552000999\n",
      "Average loss at step 44450 for last 100 steps: 0.789519479275\n",
      "Average loss at step 44500 for last 100 steps: 0.779722883701\n",
      "Average loss at step 44550 for last 100 steps: 0.770997648239\n",
      "Average loss at step 44600 for last 100 steps: 0.811265165806\n",
      "Average loss at step 44650 for last 100 steps: 0.881478470564\n",
      "Average loss at step 44700 for last 100 steps: 0.802836662531\n",
      "Average loss at step 44750 for last 100 steps: 0.760698282719\n",
      "Average loss at step 44800 for last 100 steps: 0.951196323633\n",
      "Average loss at step 44850 for last 100 steps: 0.803561223745\n",
      "Average loss at step 44900 for last 100 steps: 0.852119842768\n",
      "Average loss at step 44950 for last 100 steps: 0.816143869162\n",
      "Average loss at step 45000 for last 100 steps: 0.775975270271\n",
      "Average loss at step 45050 for last 100 steps: 0.228992476463\n",
      "Average loss at step 45100 for last 100 steps: 0.790803563595\n",
      "Average loss at step 45150 for last 100 steps: 0.890483618975\n",
      "Average loss at step 45200 for last 100 steps: 0.816347076893\n",
      "Average loss at step 45250 for last 100 steps: 0.842466849089\n",
      "Average loss at step 45300 for last 100 steps: 0.851191540956\n",
      "Average loss at step 45350 for last 100 steps: 0.894893283844\n",
      "Average loss at step 45400 for last 100 steps: 0.813497340679\n",
      "Average loss at step 45450 for last 100 steps: 0.864762885571\n",
      "Average loss at step 45500 for last 100 steps: 0.758633189201\n",
      "Average loss at step 45550 for last 100 steps: 0.790735539198\n",
      "Average loss at step 45600 for last 100 steps: 0.833975856304\n",
      "Average loss at step 45650 for last 100 steps: 0.852080016136\n",
      "Average loss at step 45700 for last 100 steps: 0.907426419258\n",
      "Average loss at step 45750 for last 100 steps: 1.0207523644\n",
      "Average loss at step 45800 for last 100 steps: 0.845895118713\n",
      "Average loss at step 45850 for last 100 steps: 0.729109870195\n",
      "Average loss at step 45900 for last 100 steps: 0.46703750968\n",
      "Average loss at step 45950 for last 100 steps: 0.767285048962\n",
      "Average loss at step 46000 for last 100 steps: 0.934337710142\n",
      "Average loss at step 46050 for last 100 steps: 0.786592869759\n",
      "Average loss at step 46100 for last 100 steps: 0.834399980307\n",
      "Average loss at step 46150 for last 100 steps: 1.06731459379\n",
      "Average loss at step 46200 for last 100 steps: 1.10437579155\n",
      "Average loss at step 46250 for last 100 steps: 1.37648762584\n",
      "Average loss at step 46300 for last 100 steps: 0.760465662479\n",
      "Average loss at step 46350 for last 100 steps: 0.840310900211\n",
      "Average loss at step 46400 for last 100 steps: 0.855964820385\n",
      "Average loss at step 46450 for last 100 steps: 0.807144438028\n",
      "Average loss at step 46500 for last 100 steps: 0.822690958977\n",
      "Average loss at step 46550 for last 100 steps: 0.985221837759\n",
      "Average loss at step 46600 for last 100 steps: 0.88557469964\n",
      "Average loss at step 46650 for last 100 steps: 0.944956542253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 46700 for last 100 steps: 1.10509666443\n",
      "Average loss at step 46750 for last 100 steps: 0.978938977718\n",
      "Average loss at step 46800 for last 100 steps: 0.754249926805\n",
      "Average loss at step 46850 for last 100 steps: 0.847456898689\n",
      "Average loss at step 46900 for last 100 steps: 0.785873351097\n",
      "Average loss at step 46950 for last 100 steps: 0.904199504852\n",
      "Average loss at step 47000 for last 100 steps: 0.756402382851\n",
      "Average loss at step 47050 for last 100 steps: 0.760615333319\n",
      "Average loss at step 47100 for last 100 steps: 0.790859646797\n",
      "Average loss at step 47150 for last 100 steps: 0.829838562012\n",
      "Average loss at step 47200 for last 100 steps: 0.777058882713\n",
      "Average loss at step 47250 for last 100 steps: 1.18390778184\n",
      "Average loss at step 47300 for last 100 steps: 0.92462305665\n",
      "Average loss at step 47350 for last 100 steps: 0.857180091143\n",
      "Average loss at step 47400 for last 100 steps: 0.805539824963\n",
      "Average loss at step 47450 for last 100 steps: 0.898547134399\n",
      "Average loss at step 47500 for last 100 steps: 0.907924349308\n",
      "Average loss at step 47550 for last 100 steps: 0.237337975502\n",
      "Average loss at step 47600 for last 100 steps: 0.838680881262\n",
      "Average loss at step 47650 for last 100 steps: 0.796196668148\n",
      "Average loss at step 47700 for last 100 steps: 0.85541249752\n",
      "Average loss at step 47750 for last 100 steps: 0.850663537979\n",
      "Average loss at step 47800 for last 100 steps: 0.841984448433\n",
      "Average loss at step 47850 for last 100 steps: 0.786604882479\n",
      "Average loss at step 47900 for last 100 steps: 0.8595063591\n",
      "Average loss at step 47950 for last 100 steps: 0.798133378029\n",
      "Average loss at step 48000 for last 100 steps: 0.799755173922\n",
      "Average loss at step 48050 for last 100 steps: 0.796274898052\n",
      "Average loss at step 48100 for last 100 steps: 0.887050267458\n",
      "Average loss at step 48150 for last 100 steps: 0.82674197197\n",
      "Average loss at step 48200 for last 100 steps: 1.1939160645\n",
      "Average loss at step 48250 for last 100 steps: 0.763184398413\n",
      "Average loss at step 48300 for last 100 steps: 0.821783118248\n",
      "Average loss at step 48350 for last 100 steps: 0.765847672224\n",
      "Average loss at step 48400 for last 100 steps: 0.509691497087\n",
      "Average loss at step 48450 for last 100 steps: 0.797018680573\n",
      "Average loss at step 48500 for last 100 steps: 0.906796780825\n",
      "Average loss at step 48550 for last 100 steps: 0.768681929111\n",
      "Average loss at step 48600 for last 100 steps: 0.930829277039\n",
      "Average loss at step 48650 for last 100 steps: 0.776435806751\n",
      "Average loss at step 48700 for last 100 steps: 0.914618250132\n",
      "Average loss at step 48750 for last 100 steps: 0.939714138508\n",
      "Average loss at step 48800 for last 100 steps: 0.881493453979\n",
      "Average loss at step 48850 for last 100 steps: 0.778621205091\n",
      "Average loss at step 48900 for last 100 steps: 0.821091756821\n",
      "Average loss at step 48950 for last 100 steps: 0.784637818336\n",
      "Average loss at step 49000 for last 100 steps: 0.796003288031\n",
      "Average loss at step 49050 for last 100 steps: 0.777673304081\n",
      "Average loss at step 49100 for last 100 steps: 0.822579499483\n",
      "Average loss at step 49150 for last 100 steps: 0.848117787838\n",
      "Average loss at step 49200 for last 100 steps: 0.840692099333\n",
      "Average loss at step 49250 for last 100 steps: 0.778993662596\n",
      "Average loss at step 49300 for last 100 steps: 0.975834598541\n",
      "Average loss at step 49350 for last 100 steps: 0.888197352886\n",
      "Average loss at step 49400 for last 100 steps: 0.813632370234\n",
      "Average loss at step 49450 for last 100 steps: 0.825429706573\n",
      "Average loss at step 49500 for last 100 steps: 0.855381760597\n",
      "Average loss at step 49550 for last 100 steps: 0.81024702549\n",
      "Average loss at step 49600 for last 100 steps: 0.807123383284\n",
      "Average loss at step 49650 for last 100 steps: 0.940540735722\n",
      "Average loss at step 49700 for last 100 steps: 0.991331237555\n",
      "Average loss at step 49750 for last 100 steps: 1.14594777703\n",
      "Average loss at step 49800 for last 100 steps: 0.898621834517\n",
      "Average loss at step 49850 for last 100 steps: 0.746751087904\n",
      "Average loss at step 49900 for last 100 steps: 0.803047984838\n",
      "Average loss at step 49950 for last 100 steps: 0.859248217344\n",
      "Average loss at step 50000 for last 100 steps: 0.838730506897\n",
      "Average loss at step 50050 for last 100 steps: 0.293663778305\n",
      "Average loss at step 50100 for last 100 steps: 0.912499564886\n",
      "Average loss at step 50150 for last 100 steps: 0.868814638853\n",
      "Average loss at step 50200 for last 100 steps: 0.918974773884\n",
      "Average loss at step 50250 for last 100 steps: 0.856677024364\n",
      "Average loss at step 50300 for last 100 steps: 0.768226155043\n",
      "Average loss at step 50350 for last 100 steps: 0.820942878723\n",
      "Average loss at step 50400 for last 100 steps: 1.01453174233\n",
      "Average loss at step 50450 for last 100 steps: 0.84810182929\n",
      "Average loss at step 50500 for last 100 steps: 0.865340445042\n",
      "Average loss at step 50550 for last 100 steps: 0.896229435205\n",
      "Average loss at step 50600 for last 100 steps: 0.810831933022\n",
      "Average loss at step 50650 for last 100 steps: 0.794338678122\n",
      "Average loss at step 50700 for last 100 steps: 0.967998408079\n",
      "Average loss at step 50750 for last 100 steps: 0.871851680279\n",
      "Average loss at step 50800 for last 100 steps: 0.831491986513\n",
      "Average loss at step 50850 for last 100 steps: 0.876745525599\n",
      "Average loss at step 50900 for last 100 steps: 0.389596489668\n",
      "Average loss at step 50950 for last 100 steps: 0.864058773518\n",
      "Average loss at step 51000 for last 100 steps: 0.860003972054\n",
      "Average loss at step 51050 for last 100 steps: 0.749923849106\n",
      "Average loss at step 51100 for last 100 steps: 0.835228719711\n",
      "Average loss at step 51150 for last 100 steps: 0.905169762373\n",
      "Average loss at step 51200 for last 100 steps: 0.921619627476\n",
      "Average loss at step 51250 for last 100 steps: 0.768742692471\n",
      "Average loss at step 51300 for last 100 steps: 0.808056204319\n",
      "Average loss at step 51350 for last 100 steps: 0.931477030516\n",
      "Average loss at step 51400 for last 100 steps: 0.831776283979\n",
      "Average loss at step 51450 for last 100 steps: 1.14996622443\n",
      "Average loss at step 51500 for last 100 steps: 1.07315272689\n",
      "Average loss at step 51550 for last 100 steps: 1.24481964588\n",
      "Average loss at step 51600 for last 100 steps: 0.807100666761\n",
      "Average loss at step 51650 for last 100 steps: 0.804842165709\n",
      "Average loss at step 51700 for last 100 steps: 0.801245312691\n",
      "Average loss at step 51750 for last 100 steps: 0.718388041258\n",
      "Average loss at step 51800 for last 100 steps: 0.744403107166\n",
      "Average loss at step 51850 for last 100 steps: 0.749230053425\n",
      "Average loss at step 51900 for last 100 steps: 0.831480544806\n",
      "Average loss at step 51950 for last 100 steps: 0.867999582291\n",
      "Average loss at step 52000 for last 100 steps: 0.912696764469\n",
      "Average loss at step 52050 for last 100 steps: 1.14923709631\n",
      "Average loss at step 52100 for last 100 steps: 0.783547327518\n",
      "Average loss at step 52150 for last 100 steps: 0.907223526239\n",
      "Average loss at step 52200 for last 100 steps: 0.967450435162\n",
      "Average loss at step 52250 for last 100 steps: 0.966513943672\n",
      "Average loss at step 52300 for last 100 steps: 0.79991568923\n",
      "Average loss at step 52350 for last 100 steps: 0.861142661572\n",
      "Average loss at step 52400 for last 100 steps: 0.774101743698\n",
      "Average loss at step 52450 for last 100 steps: 0.799474443197\n",
      "Average loss at step 52500 for last 100 steps: 0.782440705299\n",
      "Average loss at step 52550 for last 100 steps: 0.170900896788\n",
      "Average loss at step 52600 for last 100 steps: 0.764614259005\n",
      "Average loss at step 52650 for last 100 steps: 0.760985151529\n",
      "Average loss at step 52700 for last 100 steps: 0.839420082569\n",
      "Average loss at step 52750 for last 100 steps: 0.880304101706\n",
      "Average loss at step 52800 for last 100 steps: 0.797844047546\n",
      "Average loss at step 52850 for last 100 steps: 0.868727052212\n",
      "Average loss at step 52900 for last 100 steps: 0.790784907341\n",
      "Average loss at step 52950 for last 100 steps: 0.801910539865\n",
      "Average loss at step 53000 for last 100 steps: 0.92519457221\n",
      "Average loss at step 53050 for last 100 steps: 0.808635647297\n",
      "Average loss at step 53100 for last 100 steps: 0.808867362738\n",
      "Average loss at step 53150 for last 100 steps: 0.847382979393\n",
      "Average loss at step 53200 for last 100 steps: 0.806757484674\n",
      "Average loss at step 53250 for last 100 steps: 0.900172631741\n",
      "Average loss at step 53300 for last 100 steps: 0.864957002401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 53350 for last 100 steps: 0.805324915648\n",
      "Average loss at step 53400 for last 100 steps: 0.380095133781\n",
      "Average loss at step 53450 for last 100 steps: 0.957377306223\n",
      "Average loss at step 53500 for last 100 steps: 0.753596067429\n",
      "Average loss at step 53550 for last 100 steps: 0.791601967812\n",
      "Average loss at step 53600 for last 100 steps: 0.86638525486\n",
      "Average loss at step 53650 for last 100 steps: 0.867479436398\n",
      "Average loss at step 53700 for last 100 steps: 0.883715074062\n",
      "Average loss at step 53750 for last 100 steps: 0.902258192301\n",
      "Average loss at step 53800 for last 100 steps: 0.887331103086\n",
      "Average loss at step 53850 for last 100 steps: 0.917992206812\n",
      "Average loss at step 53900 for last 100 steps: 0.938626418114\n",
      "Average loss at step 53950 for last 100 steps: 0.790489383936\n",
      "Average loss at step 54000 for last 100 steps: 0.761086672544\n",
      "Average loss at step 54050 for last 100 steps: 0.781699764729\n",
      "Average loss at step 54100 for last 100 steps: 0.756934356689\n",
      "Average loss at step 54150 for last 100 steps: 0.85504379034\n",
      "Average loss at step 54200 for last 100 steps: 1.13019559622\n",
      "Average loss at step 54250 for last 100 steps: 0.634220309258\n",
      "Average loss at step 54300 for last 100 steps: 0.885171265602\n",
      "Average loss at step 54350 for last 100 steps: 0.819802286625\n",
      "Average loss at step 54400 for last 100 steps: 0.880074999332\n",
      "Average loss at step 54450 for last 100 steps: 0.99070797801\n",
      "Average loss at step 54500 for last 100 steps: 0.791074620485\n",
      "Average loss at step 54550 for last 100 steps: 0.764123197794\n",
      "Average loss at step 54600 for last 100 steps: 0.824959614277\n",
      "Average loss at step 54650 for last 100 steps: 0.80077396512\n",
      "Average loss at step 54700 for last 100 steps: 1.21787486434\n",
      "Average loss at step 54750 for last 100 steps: 0.877682743073\n",
      "Average loss at step 54800 for last 100 steps: 0.784296358824\n",
      "Average loss at step 54850 for last 100 steps: 0.79147731185\n",
      "Average loss at step 54900 for last 100 steps: 0.829820005894\n",
      "Average loss at step 54950 for last 100 steps: 0.795346204042\n",
      "Average loss at step 55000 for last 100 steps: 0.896905037165\n",
      "Average loss at step 55050 for last 100 steps: 0.102872598171\n",
      "Average loss at step 55100 for last 100 steps: 0.774406116009\n",
      "Average loss at step 55150 for last 100 steps: 0.805730261803\n",
      "Average loss at step 55200 for last 100 steps: 0.943709925413\n",
      "Average loss at step 55250 for last 100 steps: 0.785695295334\n",
      "Average loss at step 55300 for last 100 steps: 0.788550722599\n",
      "Average loss at step 55350 for last 100 steps: 0.79263967514\n",
      "Average loss at step 55400 for last 100 steps: 0.788473892212\n",
      "Average loss at step 55450 for last 100 steps: 0.766630730629\n",
      "Average loss at step 55500 for last 100 steps: 0.833026534319\n",
      "Average loss at step 55550 for last 100 steps: 0.795725277662\n",
      "Average loss at step 55600 for last 100 steps: 0.789790438414\n",
      "Average loss at step 55650 for last 100 steps: 0.889356800318\n",
      "Average loss at step 55700 for last 100 steps: 0.812442088127\n",
      "Average loss at step 55750 for last 100 steps: 0.833994070292\n",
      "Average loss at step 55800 for last 100 steps: 0.759478218555\n",
      "Average loss at step 55850 for last 100 steps: 0.921213806868\n",
      "Average loss at step 55900 for last 100 steps: 0.390502125025\n",
      "Average loss at step 55950 for last 100 steps: 0.935832307339\n",
      "Average loss at step 56000 for last 100 steps: 0.832964454889\n",
      "Average loss at step 56050 for last 100 steps: 0.824237517118\n",
      "Average loss at step 56100 for last 100 steps: 0.853246558905\n",
      "Average loss at step 56150 for last 100 steps: 0.852860147953\n",
      "Average loss at step 56200 for last 100 steps: 0.891704598665\n",
      "Average loss at step 56250 for last 100 steps: 0.888062083721\n",
      "Average loss at step 56300 for last 100 steps: 0.895207147598\n",
      "Average loss at step 56350 for last 100 steps: 0.917625426054\n",
      "Average loss at step 56400 for last 100 steps: 0.959522221088\n",
      "Average loss at step 56450 for last 100 steps: 0.760964952707\n",
      "Average loss at step 56500 for last 100 steps: 0.844767309427\n",
      "Average loss at step 56550 for last 100 steps: 0.800679898262\n",
      "Average loss at step 56600 for last 100 steps: 0.885090618134\n",
      "Average loss at step 56650 for last 100 steps: 0.944640572071\n",
      "Average loss at step 56700 for last 100 steps: 1.04131016612\n",
      "Average loss at step 56750 for last 100 steps: 0.593796722889\n",
      "Average loss at step 56800 for last 100 steps: 0.734616487026\n",
      "Average loss at step 56850 for last 100 steps: 0.830344361067\n",
      "Average loss at step 56900 for last 100 steps: 0.85641518116\n",
      "Average loss at step 56950 for last 100 steps: 0.73515250802\n",
      "Average loss at step 57000 for last 100 steps: 0.849567085505\n",
      "Average loss at step 57050 for last 100 steps: 0.862711498737\n",
      "Average loss at step 57100 for last 100 steps: 0.868066827059\n",
      "Average loss at step 57150 for last 100 steps: 0.768572587967\n",
      "Average loss at step 57200 for last 100 steps: 0.92311171174\n",
      "Average loss at step 57250 for last 100 steps: 1.02444785833\n",
      "Average loss at step 57300 for last 100 steps: 0.873779991865\n",
      "Average loss at step 57350 for last 100 steps: 0.838116409779\n",
      "Average loss at step 57400 for last 100 steps: 0.785907623768\n",
      "Average loss at step 57450 for last 100 steps: 0.865534464121\n",
      "Average loss at step 57500 for last 100 steps: 0.747856940031\n",
      "Average loss at step 57550 for last 100 steps: 0.0971126961708\n",
      "Average loss at step 57600 for last 100 steps: 0.860259250402\n",
      "Average loss at step 57650 for last 100 steps: 0.797488837242\n",
      "Average loss at step 57700 for last 100 steps: 0.88528321743\n",
      "Average loss at step 57750 for last 100 steps: 0.779668343067\n",
      "Average loss at step 57800 for last 100 steps: 0.816043487787\n",
      "Average loss at step 57850 for last 100 steps: 0.901329311132\n",
      "Average loss at step 57900 for last 100 steps: 0.975129649639\n",
      "Average loss at step 57950 for last 100 steps: 0.778346565962\n",
      "Average loss at step 58000 for last 100 steps: 0.967612909079\n",
      "Average loss at step 58050 for last 100 steps: 0.95203953743\n",
      "Average loss at step 58100 for last 100 steps: 0.93340575695\n",
      "Average loss at step 58150 for last 100 steps: 0.736001141071\n",
      "Average loss at step 58200 for last 100 steps: 0.856772047281\n",
      "Average loss at step 58250 for last 100 steps: 0.922360240221\n",
      "Average loss at step 58300 for last 100 steps: 1.01311499953\n",
      "Average loss at step 58350 for last 100 steps: 0.816349642277\n",
      "Average loss at step 58400 for last 100 steps: 0.291874074936\n",
      "Average loss at step 58450 for last 100 steps: 0.826800558567\n",
      "Average loss at step 58500 for last 100 steps: 0.947308080196\n",
      "Average loss at step 58550 for last 100 steps: 0.774893096685\n",
      "Average loss at step 58600 for last 100 steps: 0.914150588512\n",
      "Average loss at step 58650 for last 100 steps: 0.946672918797\n",
      "Average loss at step 58700 for last 100 steps: 0.887159092426\n",
      "Average loss at step 58750 for last 100 steps: 0.912914096117\n",
      "Average loss at step 58800 for last 100 steps: 0.769999296665\n",
      "Average loss at step 58850 for last 100 steps: 0.861630003452\n",
      "Average loss at step 58900 for last 100 steps: 0.747738105059\n",
      "Average loss at step 58950 for last 100 steps: 1.17523604035\n",
      "Average loss at step 59000 for last 100 steps: 0.990165507793\n",
      "Average loss at step 59050 for last 100 steps: 0.817075186968\n",
      "Average loss at step 59100 for last 100 steps: 0.790155668259\n",
      "Average loss at step 59150 for last 100 steps: 0.804990086555\n",
      "Average loss at step 59200 for last 100 steps: 0.812791881561\n",
      "Average loss at step 59250 for last 100 steps: 0.624281075001\n",
      "Average loss at step 59300 for last 100 steps: 0.853828486204\n",
      "Average loss at step 59350 for last 100 steps: 0.824089423418\n",
      "Average loss at step 59400 for last 100 steps: 0.788836148977\n",
      "Average loss at step 59450 for last 100 steps: 0.953666138649\n",
      "Average loss at step 59500 for last 100 steps: 0.871589339972\n",
      "Average loss at step 59550 for last 100 steps: 0.86984210968\n",
      "Average loss at step 59600 for last 100 steps: 0.929514343739\n",
      "Average loss at step 59650 for last 100 steps: 0.742793720961\n",
      "Average loss at step 59700 for last 100 steps: 0.975256863832\n",
      "Average loss at step 59750 for last 100 steps: 0.835415718555\n",
      "Average loss at step 59800 for last 100 steps: 1.05400960326\n",
      "Average loss at step 59850 for last 100 steps: 0.956000653505\n",
      "Average loss at step 59900 for last 100 steps: 0.774604234695\n",
      "Average loss at step 59950 for last 100 steps: 0.924279900789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 60000 for last 100 steps: 0.91456474185\n",
      "Average loss at step 60050 for last 100 steps: 0.0276050102711\n",
      "Average loss at step 60100 for last 100 steps: 0.764664051533\n",
      "Average loss at step 60150 for last 100 steps: 0.856839647293\n",
      "Average loss at step 60200 for last 100 steps: 0.751307467222\n",
      "Average loss at step 60250 for last 100 steps: 0.772349522114\n",
      "Average loss at step 60300 for last 100 steps: 0.800594222546\n",
      "Average loss at step 60350 for last 100 steps: 0.841287105083\n",
      "Average loss at step 60400 for last 100 steps: 0.847219592333\n",
      "Average loss at step 60450 for last 100 steps: 0.808486229181\n",
      "Average loss at step 60500 for last 100 steps: 0.781887404919\n",
      "Average loss at step 60550 for last 100 steps: 0.938682091236\n",
      "Average loss at step 60600 for last 100 steps: 0.787795460224\n",
      "Average loss at step 60650 for last 100 steps: 0.790819694996\n",
      "Average loss at step 60700 for last 100 steps: 0.800205214024\n",
      "Average loss at step 60750 for last 100 steps: 0.748110390902\n",
      "Average loss at step 60800 for last 100 steps: 0.821420217752\n",
      "Average loss at step 60850 for last 100 steps: 0.799320890903\n",
      "Average loss at step 60900 for last 100 steps: 0.263928407431\n",
      "Average loss at step 60950 for last 100 steps: 0.812954689264\n",
      "Average loss at step 61000 for last 100 steps: 0.783816518784\n",
      "Average loss at step 61050 for last 100 steps: 0.986339234114\n",
      "Average loss at step 61100 for last 100 steps: 0.809640855789\n",
      "Average loss at step 61150 for last 100 steps: 0.787739008665\n",
      "Average loss at step 61200 for last 100 steps: 1.20614818573\n",
      "Average loss at step 61250 for last 100 steps: 0.846084153652\n",
      "Average loss at step 61300 for last 100 steps: 0.74930385828\n",
      "Average loss at step 61350 for last 100 steps: 0.783566955328\n",
      "Average loss at step 61400 for last 100 steps: 0.783690326214\n",
      "Average loss at step 61450 for last 100 steps: 0.795286133289\n",
      "Average loss at step 61500 for last 100 steps: 1.05754532099\n",
      "Average loss at step 61550 for last 100 steps: 0.987675453424\n",
      "Average loss at step 61600 for last 100 steps: 0.826868228912\n",
      "Average loss at step 61650 for last 100 steps: 0.86128210783\n",
      "Average loss at step 61700 for last 100 steps: 1.02193294883\n",
      "Average loss at step 61750 for last 100 steps: 0.598275399208\n",
      "Average loss at step 61800 for last 100 steps: 0.870097026825\n",
      "Average loss at step 61850 for last 100 steps: 1.08379350781\n",
      "Average loss at step 61900 for last 100 steps: 0.933605730534\n",
      "Average loss at step 61950 for last 100 steps: 0.832093864679\n",
      "Average loss at step 62000 for last 100 steps: 0.786482924223\n",
      "Average loss at step 62050 for last 100 steps: 0.820901430845\n",
      "Average loss at step 62100 for last 100 steps: 0.831664475203\n",
      "Average loss at step 62150 for last 100 steps: 0.811250550747\n",
      "Average loss at step 62200 for last 100 steps: 0.746507372856\n",
      "Average loss at step 62250 for last 100 steps: 0.862871553898\n",
      "Average loss at step 62300 for last 100 steps: 0.786686466932\n",
      "Average loss at step 62350 for last 100 steps: 0.797681227922\n",
      "Average loss at step 62400 for last 100 steps: 0.739959996939\n",
      "Average loss at step 62450 for last 100 steps: 0.996087633371\n",
      "Average loss at step 62500 for last 100 steps: 0.859437215328\n",
      "Average loss at step 62550 for last 100 steps: 1.07920741796\n",
      "Average loss at step 62600 for last 100 steps: 0.805040686131\n",
      "Average loss at step 62650 for last 100 steps: 0.887737503052\n",
      "Average loss at step 62700 for last 100 steps: 0.832768806219\n",
      "Average loss at step 62750 for last 100 steps: 0.834229125977\n",
      "Average loss at step 62800 for last 100 steps: 0.855956773758\n",
      "Average loss at step 62850 for last 100 steps: 0.974059574604\n",
      "Average loss at step 62900 for last 100 steps: 1.02161288261\n",
      "Average loss at step 62950 for last 100 steps: 0.737479379177\n",
      "Average loss at step 63000 for last 100 steps: 0.752039817572\n",
      "Average loss at step 63050 for last 100 steps: 0.878802393675\n",
      "Average loss at step 63100 for last 100 steps: 1.03648544312\n",
      "Average loss at step 63150 for last 100 steps: 0.80724734664\n",
      "Average loss at step 63200 for last 100 steps: 0.778902848959\n",
      "Average loss at step 63250 for last 100 steps: 0.877307610512\n",
      "Average loss at step 63300 for last 100 steps: 0.812703917027\n",
      "Average loss at step 63350 for last 100 steps: 0.804603329897\n",
      "Average loss at step 63400 for last 100 steps: 0.273154057264\n",
      "Average loss at step 63450 for last 100 steps: 0.879770393372\n",
      "Average loss at step 63500 for last 100 steps: 0.877004288435\n",
      "Average loss at step 63550 for last 100 steps: 0.764062663317\n",
      "Average loss at step 63600 for last 100 steps: 1.10664750457\n",
      "Average loss at step 63650 for last 100 steps: 0.91053699851\n",
      "Average loss at step 63700 for last 100 steps: 0.786280896664\n",
      "Average loss at step 63750 for last 100 steps: 0.908207535744\n",
      "Average loss at step 63800 for last 100 steps: 1.00293677926\n",
      "Average loss at step 63850 for last 100 steps: 0.873797547817\n",
      "Average loss at step 63900 for last 100 steps: 0.923240309954\n",
      "Average loss at step 63950 for last 100 steps: 0.805472147465\n",
      "Average loss at step 64000 for last 100 steps: 0.908729704618\n",
      "Average loss at step 64050 for last 100 steps: 0.774942147732\n",
      "Average loss at step 64100 for last 100 steps: 0.814502309561\n",
      "Average loss at step 64150 for last 100 steps: 0.884829239845\n",
      "Average loss at step 64200 for last 100 steps: 0.755041803122\n",
      "Average loss at step 64250 for last 100 steps: 0.534524559975\n",
      "Average loss at step 64300 for last 100 steps: 0.840238921642\n",
      "Average loss at step 64350 for last 100 steps: 0.97639580369\n",
      "Average loss at step 64400 for last 100 steps: 0.815373198986\n",
      "Average loss at step 64450 for last 100 steps: 0.768075977564\n",
      "Average loss at step 64500 for last 100 steps: 0.811445896626\n",
      "Average loss at step 64550 for last 100 steps: 0.91279619813\n",
      "Average loss at step 64600 for last 100 steps: 0.793236646652\n",
      "Average loss at step 64650 for last 100 steps: 0.766350864172\n",
      "Average loss at step 64700 for last 100 steps: 0.796826155186\n",
      "Average loss at step 64750 for last 100 steps: 0.856661328077\n",
      "Average loss at step 64800 for last 100 steps: 0.755519586802\n",
      "Average loss at step 64850 for last 100 steps: 0.821426484585\n",
      "Average loss at step 64900 for last 100 steps: 1.25667996168\n",
      "Average loss at step 64950 for last 100 steps: 0.854286420345\n",
      "Average loss at step 65000 for last 100 steps: 0.908480547667\n",
      "Average loss at step 65050 for last 100 steps: 0.915722900629\n",
      "Average loss at step 65100 for last 100 steps: 0.896821365356\n",
      "Average loss at step 65150 for last 100 steps: 1.03018636584\n",
      "Average loss at step 65200 for last 100 steps: 0.733969731331\n",
      "Average loss at step 65250 for last 100 steps: 0.885873544216\n",
      "Average loss at step 65300 for last 100 steps: 0.970369255543\n",
      "Average loss at step 65350 for last 100 steps: 1.05362940788\n",
      "Average loss at step 65400 for last 100 steps: 0.75845341444\n",
      "Average loss at step 65450 for last 100 steps: 0.824291697741\n",
      "Average loss at step 65500 for last 100 steps: 0.813074842691\n",
      "Average loss at step 65550 for last 100 steps: 0.748553255796\n",
      "Average loss at step 65600 for last 100 steps: 1.09555781245\n",
      "Average loss at step 65650 for last 100 steps: 0.84853749156\n",
      "Average loss at step 65700 for last 100 steps: 0.936892753839\n",
      "Average loss at step 65750 for last 100 steps: 1.01092612267\n",
      "Average loss at step 65800 for last 100 steps: 0.872643370628\n",
      "Average loss at step 65850 for last 100 steps: 0.767793755531\n",
      "Average loss at step 65900 for last 100 steps: 0.232994260788\n",
      "Average loss at step 65950 for last 100 steps: 0.886965631247\n",
      "Average loss at step 66000 for last 100 steps: 0.770603113174\n",
      "Average loss at step 66050 for last 100 steps: 0.868038544655\n",
      "Average loss at step 66100 for last 100 steps: 0.804340233803\n",
      "Average loss at step 66150 for last 100 steps: 0.765581997633\n",
      "Average loss at step 66200 for last 100 steps: 0.836236391068\n",
      "Average loss at step 66250 for last 100 steps: 0.902018121481\n",
      "Average loss at step 66300 for last 100 steps: 0.848220230341\n",
      "Average loss at step 66350 for last 100 steps: 0.797610127926\n",
      "Average loss at step 66400 for last 100 steps: 0.852360379696\n",
      "Average loss at step 66450 for last 100 steps: 0.83838136673\n",
      "Average loss at step 66500 for last 100 steps: 0.760113024712\n",
      "Average loss at step 66550 for last 100 steps: 0.791711307764\n",
      "Average loss at step 66600 for last 100 steps: 0.931252808571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 66650 for last 100 steps: 0.775708465576\n",
      "Average loss at step 66700 for last 100 steps: 0.827561095953\n",
      "Average loss at step 66750 for last 100 steps: 0.506326293945\n",
      "Average loss at step 66800 for last 100 steps: 0.81321395874\n",
      "Average loss at step 66850 for last 100 steps: 0.787022610903\n",
      "Average loss at step 66900 for last 100 steps: 0.898806549311\n",
      "Average loss at step 66950 for last 100 steps: 0.971841423512\n",
      "Average loss at step 67000 for last 100 steps: 0.835799255371\n",
      "Average loss at step 67050 for last 100 steps: 0.833322433233\n",
      "Average loss at step 67100 for last 100 steps: 1.15050042033\n",
      "Average loss at step 67150 for last 100 steps: 0.78180711627\n",
      "Average loss at step 67200 for last 100 steps: 0.77956599474\n",
      "Average loss at step 67250 for last 100 steps: 0.938865323067\n",
      "Average loss at step 67300 for last 100 steps: 0.794261894226\n",
      "Average loss at step 67350 for last 100 steps: 0.791354432106\n",
      "Average loss at step 67400 for last 100 steps: 0.906051640511\n",
      "Average loss at step 67450 for last 100 steps: 0.885379822254\n",
      "Average loss at step 67500 for last 100 steps: 0.823697851896\n",
      "Average loss at step 67550 for last 100 steps: 0.807116799355\n",
      "Average loss at step 67600 for last 100 steps: 0.777105476856\n",
      "Average loss at step 67650 for last 100 steps: 0.960038167238\n",
      "Average loss at step 67700 for last 100 steps: 0.805304940939\n",
      "Average loss at step 67750 for last 100 steps: 0.81303157568\n",
      "Average loss at step 67800 for last 100 steps: 0.809612249136\n",
      "Average loss at step 67850 for last 100 steps: 0.747681918144\n",
      "Average loss at step 67900 for last 100 steps: 0.787059209347\n",
      "Average loss at step 67950 for last 100 steps: 0.902560720444\n",
      "Average loss at step 68000 for last 100 steps: 0.822662500143\n",
      "Average loss at step 68050 for last 100 steps: 0.802314457893\n",
      "Average loss at step 68100 for last 100 steps: 0.916164289713\n",
      "Average loss at step 68150 for last 100 steps: 0.789986302853\n",
      "Average loss at step 68200 for last 100 steps: 0.798602004051\n",
      "Average loss at step 68250 for last 100 steps: 0.847821391821\n",
      "Average loss at step 68300 for last 100 steps: 0.750420489311\n",
      "Average loss at step 68350 for last 100 steps: 0.814879151583\n",
      "Average loss at step 68400 for last 100 steps: 0.182593650818\n",
      "Average loss at step 68450 for last 100 steps: 0.85945913434\n",
      "Average loss at step 68500 for last 100 steps: 0.773363280296\n",
      "Average loss at step 68550 for last 100 steps: 0.948828668594\n",
      "Average loss at step 68600 for last 100 steps: 0.755893579721\n",
      "Average loss at step 68650 for last 100 steps: 0.813366670609\n",
      "Average loss at step 68700 for last 100 steps: 0.850720411539\n",
      "Average loss at step 68750 for last 100 steps: 0.818471945524\n",
      "Average loss at step 68800 for last 100 steps: 0.883102344275\n",
      "Average loss at step 68850 for last 100 steps: 1.04538703203\n",
      "Average loss at step 68900 for last 100 steps: 1.01900587678\n",
      "Average loss at step 68950 for last 100 steps: 0.785860054493\n",
      "Average loss at step 69000 for last 100 steps: 0.735515280962\n",
      "Average loss at step 69050 for last 100 steps: 0.773213510513\n",
      "Average loss at step 69100 for last 100 steps: 0.758802244663\n",
      "Average loss at step 69150 for last 100 steps: 0.868846663237\n",
      "Average loss at step 69200 for last 100 steps: 0.84699165225\n",
      "Average loss at step 69250 for last 100 steps: 0.441657335758\n",
      "Average loss at step 69300 for last 100 steps: 0.797435588837\n",
      "Average loss at step 69350 for last 100 steps: 0.78427993536\n",
      "Average loss at step 69400 for last 100 steps: 0.799836592674\n",
      "Average loss at step 69450 for last 100 steps: 0.913428477049\n",
      "Average loss at step 69500 for last 100 steps: 1.00092630029\n",
      "Average loss at step 69550 for last 100 steps: 0.83153942585\n",
      "Average loss at step 69600 for last 100 steps: 0.85208239913\n",
      "Average loss at step 69650 for last 100 steps: 0.822980211973\n",
      "Average loss at step 69700 for last 100 steps: 0.795275044441\n",
      "Average loss at step 69750 for last 100 steps: 0.875478947163\n",
      "Average loss at step 69800 for last 100 steps: 0.803712453842\n",
      "Average loss at step 69850 for last 100 steps: 0.842132899761\n",
      "Average loss at step 69900 for last 100 steps: 0.80881201148\n",
      "Average loss at step 69950 for last 100 steps: 0.795039110184\n",
      "Average loss at step 70000 for last 100 steps: 0.881786260605\n",
      "Average loss at step 70050 for last 100 steps: 0.826203058958\n",
      "Average loss at step 70100 for last 100 steps: 0.84807410717\n",
      "Average loss at step 70150 for last 100 steps: 0.818217703104\n",
      "Average loss at step 70200 for last 100 steps: 0.830272382498\n",
      "Average loss at step 70250 for last 100 steps: 0.789844583273\n",
      "Average loss at step 70300 for last 100 steps: 0.856885797977\n",
      "Average loss at step 70350 for last 100 steps: 0.936166695356\n",
      "Average loss at step 70400 for last 100 steps: 0.958450978994\n",
      "Average loss at step 70450 for last 100 steps: 0.883352525234\n",
      "Average loss at step 70500 for last 100 steps: 0.948812875748\n",
      "Average loss at step 70550 for last 100 steps: 0.822215656042\n",
      "Average loss at step 70600 for last 100 steps: 0.830297279358\n",
      "Average loss at step 70650 for last 100 steps: 0.983681524992\n",
      "Average loss at step 70700 for last 100 steps: 0.841155114174\n",
      "Average loss at step 70750 for last 100 steps: 0.965655816793\n",
      "Average loss at step 70800 for last 100 steps: 0.921294237375\n",
      "Average loss at step 70850 for last 100 steps: 0.853905148506\n",
      "Average loss at step 70900 for last 100 steps: 0.191775977612\n",
      "Average loss at step 70950 for last 100 steps: 0.783812114\n",
      "Average loss at step 71000 for last 100 steps: 0.923507188559\n",
      "Average loss at step 71050 for last 100 steps: 0.74892896533\n",
      "Average loss at step 71100 for last 100 steps: 0.882655205727\n",
      "Average loss at step 71150 for last 100 steps: 1.01723253727\n",
      "Average loss at step 71200 for last 100 steps: 1.0612827158\n",
      "Average loss at step 71250 for last 100 steps: 0.801406127214\n",
      "Average loss at step 71300 for last 100 steps: 0.904271876812\n",
      "Average loss at step 71350 for last 100 steps: 0.809842735529\n",
      "Average loss at step 71400 for last 100 steps: 0.806601229906\n",
      "Average loss at step 71450 for last 100 steps: 0.834982110262\n",
      "Average loss at step 71500 for last 100 steps: 0.796099561453\n",
      "Average loss at step 71550 for last 100 steps: 0.851775461435\n",
      "Average loss at step 71600 for last 100 steps: 0.748978503942\n",
      "Average loss at step 71650 for last 100 steps: 0.800036836863\n",
      "Average loss at step 71700 for last 100 steps: 0.89723200798\n",
      "Average loss at step 71750 for last 100 steps: 0.422639914751\n",
      "Average loss at step 71800 for last 100 steps: 0.878702040911\n",
      "Average loss at step 71850 for last 100 steps: 0.756591924429\n",
      "Average loss at step 71900 for last 100 steps: 0.805697104931\n",
      "Average loss at step 71950 for last 100 steps: 0.804958460331\n",
      "Average loss at step 72000 for last 100 steps: 0.994469841719\n",
      "Average loss at step 72050 for last 100 steps: 0.913905630112\n",
      "Average loss at step 72100 for last 100 steps: 0.811013942957\n",
      "Average loss at step 72150 for last 100 steps: 0.861346265078\n",
      "Average loss at step 72200 for last 100 steps: 0.844832093716\n",
      "Average loss at step 72250 for last 100 steps: 0.779773501158\n",
      "Average loss at step 72300 for last 100 steps: 0.774474544525\n",
      "Average loss at step 72350 for last 100 steps: 0.938456586599\n",
      "Average loss at step 72400 for last 100 steps: 0.799377430677\n",
      "Average loss at step 72450 for last 100 steps: 0.869592484236\n",
      "Average loss at step 72500 for last 100 steps: 0.873562150002\n",
      "Average loss at step 72550 for last 100 steps: 1.02343458891\n",
      "Average loss at step 72600 for last 100 steps: 0.686389684677\n",
      "Average loss at step 72650 for last 100 steps: 0.976497335434\n",
      "Average loss at step 72700 for last 100 steps: 0.916564596891\n",
      "Average loss at step 72750 for last 100 steps: 0.772392477989\n",
      "Average loss at step 72800 for last 100 steps: 0.97032333374\n",
      "Average loss at step 72850 for last 100 steps: 0.849006737471\n",
      "Average loss at step 72900 for last 100 steps: 0.768375936747\n",
      "Average loss at step 72950 for last 100 steps: 0.763952937126\n",
      "Average loss at step 73000 for last 100 steps: 0.92924708128\n",
      "Average loss at step 73050 for last 100 steps: 0.898816107512\n",
      "Average loss at step 73100 for last 100 steps: 0.823602126837\n",
      "Average loss at step 73150 for last 100 steps: 0.950700092316\n",
      "Average loss at step 73200 for last 100 steps: 0.878688695431\n",
      "Average loss at step 73250 for last 100 steps: 0.855406628847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 73300 for last 100 steps: 0.7405527246\n",
      "Average loss at step 73350 for last 100 steps: 0.760439784527\n",
      "Average loss at step 73400 for last 100 steps: 0.130562577248\n",
      "Average loss at step 73450 for last 100 steps: 0.865636115074\n",
      "Average loss at step 73500 for last 100 steps: 0.854081935883\n",
      "Average loss at step 73550 for last 100 steps: 0.863090543747\n",
      "Average loss at step 73600 for last 100 steps: 0.886587232351\n",
      "Average loss at step 73650 for last 100 steps: 0.78847899437\n",
      "Average loss at step 73700 for last 100 steps: 0.75795609355\n",
      "Average loss at step 73750 for last 100 steps: 0.773727880716\n",
      "Average loss at step 73800 for last 100 steps: 0.797227808237\n",
      "Average loss at step 73850 for last 100 steps: 1.2346457839\n",
      "Average loss at step 73900 for last 100 steps: 0.874172064066\n",
      "Average loss at step 73950 for last 100 steps: 0.765183807611\n",
      "Average loss at step 74000 for last 100 steps: 0.825729734898\n",
      "Average loss at step 74050 for last 100 steps: 0.864796862602\n",
      "Average loss at step 74100 for last 100 steps: 1.01769559503\n",
      "Average loss at step 74150 for last 100 steps: 0.839837824106\n",
      "Average loss at step 74200 for last 100 steps: 0.8598567307\n",
      "Average loss at step 74250 for last 100 steps: 0.515027703047\n",
      "Average loss at step 74300 for last 100 steps: 0.842509850264\n",
      "Average loss at step 74350 for last 100 steps: 0.762640327215\n",
      "Average loss at step 74400 for last 100 steps: 0.882956705093\n",
      "Average loss at step 74450 for last 100 steps: 0.890247731209\n",
      "Average loss at step 74500 for last 100 steps: 0.762507522106\n",
      "Average loss at step 74550 for last 100 steps: 0.86074942708\n",
      "Average loss at step 74600 for last 100 steps: 0.950917485952\n",
      "Average loss at step 74650 for last 100 steps: 0.777757917643\n",
      "Average loss at step 74700 for last 100 steps: 0.884510476589\n",
      "Average loss at step 74750 for last 100 steps: 0.803984287977\n",
      "Average loss at step 74800 for last 100 steps: 0.78891463995\n",
      "Average loss at step 74850 for last 100 steps: 0.812434272766\n",
      "Average loss at step 74900 for last 100 steps: 0.971673871279\n",
      "Average loss at step 74950 for last 100 steps: 0.986089094877\n",
      "Average loss at step 75000 for last 100 steps: 0.788437997103\n",
      "Average loss at step 75050 for last 100 steps: 0.911816341877\n",
      "Average loss at step 75100 for last 100 steps: 0.709139686823\n",
      "Average loss at step 75150 for last 100 steps: 0.777487746477\n",
      "Average loss at step 75200 for last 100 steps: 0.768288971186\n",
      "Average loss at step 75250 for last 100 steps: 0.99104526639\n",
      "Average loss at step 75300 for last 100 steps: 0.764489506483\n",
      "Average loss at step 75350 for last 100 steps: 0.844969485998\n",
      "Average loss at step 75400 for last 100 steps: 0.799787157774\n",
      "Average loss at step 75450 for last 100 steps: 0.927680133581\n",
      "Average loss at step 75500 for last 100 steps: 0.792464021444\n",
      "Average loss at step 75550 for last 100 steps: 0.903505458832\n",
      "Average loss at step 75600 for last 100 steps: 1.30201545\n",
      "Average loss at step 75650 for last 100 steps: 0.829417960644\n",
      "Average loss at step 75700 for last 100 steps: 0.842761986256\n",
      "Average loss at step 75750 for last 100 steps: 0.728662178516\n",
      "Average loss at step 75800 for last 100 steps: 0.78533457756\n",
      "Average loss at step 75850 for last 100 steps: 0.90316110611\n",
      "Average loss at step 75900 for last 100 steps: 0.0856924843788\n",
      "Average loss at step 75950 for last 100 steps: 0.752299079895\n",
      "Average loss at step 76000 for last 100 steps: 0.847165834904\n",
      "Average loss at step 76050 for last 100 steps: 0.900032730103\n",
      "Average loss at step 76100 for last 100 steps: 0.814489921331\n",
      "Average loss at step 76150 for last 100 steps: 0.803476166725\n",
      "Average loss at step 76200 for last 100 steps: 0.806319545507\n",
      "Average loss at step 76250 for last 100 steps: 0.936237232685\n",
      "Average loss at step 76300 for last 100 steps: 0.924718188047\n",
      "Average loss at step 76350 for last 100 steps: 0.895199545622\n",
      "Average loss at step 76400 for last 100 steps: 0.823629468679\n",
      "Average loss at step 76450 for last 100 steps: 1.04923380017\n",
      "Average loss at step 76500 for last 100 steps: 0.800737627745\n",
      "Average loss at step 76550 for last 100 steps: 1.14783311248\n",
      "Average loss at step 76600 for last 100 steps: 1.20702225804\n",
      "Average loss at step 76650 for last 100 steps: 0.75537684679\n",
      "Average loss at step 76700 for last 100 steps: 0.864937833548\n",
      "Average loss at step 76750 for last 100 steps: 0.37105014801\n",
      "Average loss at step 76800 for last 100 steps: 0.864817712307\n",
      "Average loss at step 76850 for last 100 steps: 0.75454770565\n",
      "Average loss at step 76900 for last 100 steps: 0.859633508921\n",
      "Average loss at step 76950 for last 100 steps: 0.853296750784\n",
      "Average loss at step 77000 for last 100 steps: 0.819826425314\n",
      "Average loss at step 77050 for last 100 steps: 0.736427990198\n",
      "Average loss at step 77100 for last 100 steps: 0.794904369116\n",
      "Average loss at step 77150 for last 100 steps: 0.776790082455\n",
      "Average loss at step 77200 for last 100 steps: 0.915915877819\n",
      "Average loss at step 77250 for last 100 steps: 0.834827848673\n",
      "Average loss at step 77300 for last 100 steps: 0.848185340166\n",
      "Average loss at step 77350 for last 100 steps: 0.895117814541\n",
      "Average loss at step 77400 for last 100 steps: 0.805026179552\n",
      "Average loss at step 77450 for last 100 steps: 0.792768743038\n",
      "Average loss at step 77500 for last 100 steps: 0.804336168766\n",
      "Average loss at step 77550 for last 100 steps: 0.88514642477\n",
      "Average loss at step 77600 for last 100 steps: 0.694842020273\n",
      "Average loss at step 77650 for last 100 steps: 0.792425792217\n",
      "Average loss at step 77700 for last 100 steps: 0.866169513464\n",
      "Average loss at step 77750 for last 100 steps: 1.00641097546\n",
      "Average loss at step 77800 for last 100 steps: 0.798891298771\n",
      "Average loss at step 77850 for last 100 steps: 0.77948335886\n",
      "Average loss at step 77900 for last 100 steps: 0.969636770487\n",
      "Average loss at step 77950 for last 100 steps: 0.98028603673\n",
      "Average loss at step 78000 for last 100 steps: 1.05835438728\n",
      "Average loss at step 78050 for last 100 steps: 0.821735123396\n",
      "Average loss at step 78100 for last 100 steps: 0.767539285421\n",
      "Average loss at step 78150 for last 100 steps: 0.775318846703\n",
      "Average loss at step 78200 for last 100 steps: 0.745372625589\n",
      "Average loss at step 78250 for last 100 steps: 0.835008704662\n",
      "Average loss at step 78300 for last 100 steps: 0.953750293255\n",
      "Average loss at step 78350 for last 100 steps: 0.830136049986\n",
      "Average loss at step 78400 for last 100 steps: 0.0753495669365\n",
      "Average loss at step 78450 for last 100 steps: 1.05543501139\n",
      "Average loss at step 78500 for last 100 steps: 1.03904798508\n",
      "Average loss at step 78550 for last 100 steps: 0.899564887285\n",
      "Average loss at step 78600 for last 100 steps: 0.805789242983\n",
      "Average loss at step 78650 for last 100 steps: 0.847674148083\n",
      "Average loss at step 78700 for last 100 steps: 0.863280789852\n",
      "Average loss at step 78750 for last 100 steps: 0.785358511209\n",
      "Average loss at step 78800 for last 100 steps: 0.904898132086\n",
      "Average loss at step 78850 for last 100 steps: 0.775165314674\n",
      "Average loss at step 78900 for last 100 steps: 0.797036801577\n",
      "Average loss at step 78950 for last 100 steps: 0.738719178438\n",
      "Average loss at step 79000 for last 100 steps: 0.825036094189\n",
      "Average loss at step 79050 for last 100 steps: 0.821898332834\n",
      "Average loss at step 79100 for last 100 steps: 0.772727597952\n",
      "Average loss at step 79150 for last 100 steps: 0.816362665892\n",
      "Average loss at step 79200 for last 100 steps: 0.787885254622\n",
      "Average loss at step 79250 for last 100 steps: 0.481522624493\n",
      "Average loss at step 79300 for last 100 steps: 1.23030970335\n",
      "Average loss at step 79350 for last 100 steps: 1.03006237149\n",
      "Average loss at step 79400 for last 100 steps: 0.9083839643\n",
      "Average loss at step 79450 for last 100 steps: 0.84394562006\n",
      "Average loss at step 79500 for last 100 steps: 0.783136247396\n",
      "Average loss at step 79550 for last 100 steps: 0.77296123147\n",
      "Average loss at step 79600 for last 100 steps: 0.858434354067\n",
      "Average loss at step 79650 for last 100 steps: 0.83998061657\n",
      "Average loss at step 79700 for last 100 steps: 0.955884457827\n",
      "Average loss at step 79750 for last 100 steps: 0.970823229551\n",
      "Average loss at step 79800 for last 100 steps: 0.779543896914\n",
      "Average loss at step 79850 for last 100 steps: 0.788713611364\n",
      "Average loss at step 79900 for last 100 steps: 0.828147397041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 79950 for last 100 steps: 0.938932425976\n",
      "Average loss at step 80000 for last 100 steps: 0.853807878494\n",
      "Average loss at step 80050 for last 100 steps: 0.806108102798\n",
      "Average loss at step 80100 for last 100 steps: 0.568194856644\n",
      "Average loss at step 80150 for last 100 steps: 0.824947046041\n",
      "Average loss at step 80200 for last 100 steps: 0.789974269867\n",
      "Average loss at step 80250 for last 100 steps: 1.1792767477\n",
      "Average loss at step 80300 for last 100 steps: 0.840507316589\n",
      "Average loss at step 80350 for last 100 steps: 0.922220025063\n",
      "Average loss at step 80400 for last 100 steps: 0.94896073103\n",
      "Average loss at step 80450 for last 100 steps: 0.818103284836\n",
      "Average loss at step 80500 for last 100 steps: 0.812515683174\n",
      "Average loss at step 80550 for last 100 steps: 0.875473091602\n",
      "Average loss at step 80600 for last 100 steps: 0.858656263351\n",
      "Average loss at step 80650 for last 100 steps: 0.971578508615\n",
      "Average loss at step 80700 for last 100 steps: 0.909898412228\n",
      "Average loss at step 80750 for last 100 steps: 0.793712744713\n",
      "Average loss at step 80800 for last 100 steps: 0.756987513304\n",
      "Average loss at step 80850 for last 100 steps: 0.778126330376\n",
      "Average loss at step 80900 for last 100 steps: 0.0378841733932\n",
      "Average loss at step 80950 for last 100 steps: 0.770051521063\n",
      "Average loss at step 81000 for last 100 steps: 0.829782767296\n",
      "Average loss at step 81050 for last 100 steps: 0.918265519142\n",
      "Average loss at step 81100 for last 100 steps: 0.823939409256\n",
      "Average loss at step 81150 for last 100 steps: 0.826333523989\n",
      "Average loss at step 81200 for last 100 steps: 1.01885368943\n",
      "Average loss at step 81250 for last 100 steps: 0.893401114941\n",
      "Average loss at step 81300 for last 100 steps: 0.857523967028\n",
      "Average loss at step 81350 for last 100 steps: 0.782274781466\n",
      "Average loss at step 81400 for last 100 steps: 0.891309742928\n",
      "Average loss at step 81450 for last 100 steps: 0.91005466938\n",
      "Average loss at step 81500 for last 100 steps: 0.908189513683\n",
      "Average loss at step 81550 for last 100 steps: 0.912252651453\n",
      "Average loss at step 81600 for last 100 steps: 1.12453588724\n",
      "Average loss at step 81650 for last 100 steps: 1.02429566264\n",
      "Average loss at step 81700 for last 100 steps: 0.89307320714\n",
      "Average loss at step 81750 for last 100 steps: 0.261761987209\n",
      "Average loss at step 81800 for last 100 steps: 0.809817916155\n",
      "Average loss at step 81850 for last 100 steps: 1.00929795027\n",
      "Average loss at step 81900 for last 100 steps: 0.75814896822\n",
      "Average loss at step 81950 for last 100 steps: 0.765477819443\n",
      "Average loss at step 82000 for last 100 steps: 0.781636738777\n",
      "Average loss at step 82050 for last 100 steps: 0.837795782089\n",
      "Average loss at step 82100 for last 100 steps: 0.945464274883\n",
      "Average loss at step 82150 for last 100 steps: 1.00923954725\n",
      "Average loss at step 82200 for last 100 steps: 0.993240493536\n",
      "Average loss at step 82250 for last 100 steps: 0.881796264648\n",
      "Average loss at step 82300 for last 100 steps: 0.849605741501\n",
      "Average loss at step 82350 for last 100 steps: 1.07933333278\n",
      "Average loss at step 82400 for last 100 steps: 0.864273469448\n",
      "Average loss at step 82450 for last 100 steps: 1.01735535264\n",
      "Average loss at step 82500 for last 100 steps: 0.799218090773\n",
      "Average loss at step 82550 for last 100 steps: 0.794295105934\n",
      "Average loss at step 82600 for last 100 steps: 0.516410763264\n",
      "Average loss at step 82650 for last 100 steps: 0.810921424627\n",
      "Average loss at step 82700 for last 100 steps: 0.779832237959\n",
      "Average loss at step 82750 for last 100 steps: 0.815512472391\n",
      "Average loss at step 82800 for last 100 steps: 0.754564298391\n",
      "Average loss at step 82850 for last 100 steps: 0.831064164639\n",
      "Average loss at step 82900 for last 100 steps: 0.911850582361\n",
      "Average loss at step 82950 for last 100 steps: 0.789030538797\n",
      "Average loss at step 83000 for last 100 steps: 0.909385187626\n",
      "Average loss at step 83050 for last 100 steps: 0.765644164085\n",
      "Average loss at step 83100 for last 100 steps: 0.811412587166\n",
      "Average loss at step 83150 for last 100 steps: 1.15012463808\n",
      "Average loss at step 83200 for last 100 steps: 0.776481038332\n",
      "Average loss at step 83250 for last 100 steps: 0.76840549469\n",
      "Average loss at step 83300 for last 100 steps: 0.883659145832\n",
      "Average loss at step 83350 for last 100 steps: 0.832286154032\n",
      "Average loss at step 83400 for last 100 steps: 0.845012918711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0528638911247254,\n",
       " 0.9319792008399963,\n",
       " 0.85039432764053347,\n",
       " 0.7867298007011414,\n",
       " 0.98144269227981562,\n",
       " 0.81305771231651303,\n",
       " 0.7590865814685821,\n",
       " 0.83317650079727168,\n",
       " 0.81052036881446843,\n",
       " 0.87346493482589727,\n",
       " 1.0885330212116242,\n",
       " 0.93868502616882321,\n",
       " 0.79912550568580631,\n",
       " 0.85367266774177553,\n",
       " 0.79559267640113829,\n",
       " 0.80948040246963504,\n",
       " 0.24285692930221559,\n",
       " 0.71911592721939088,\n",
       " 1.0659229230880738,\n",
       " 0.74668430447578427,\n",
       " 0.84169348716735837,\n",
       " 0.77762767195701599,\n",
       " 0.76642739892005918,\n",
       " 0.88802905797958376,\n",
       " 0.8003810238838196,\n",
       " 0.81373012185096738,\n",
       " 0.80655039310455323,\n",
       " 0.7485801291465759,\n",
       " 0.80606162786483759,\n",
       " 0.87291187405586246,\n",
       " 0.84932038664817811,\n",
       " 1.0687380349636078,\n",
       " 1.0871946299076081,\n",
       " 0.61766077876091008,\n",
       " 0.77863313317298888,\n",
       " 0.84047901630401611,\n",
       " 0.81970775365829462,\n",
       " 0.76926356792449946,\n",
       " 0.88603796362876897,\n",
       " 1.0499413776397706,\n",
       " 0.87285285115242006,\n",
       " 0.83016181826591495,\n",
       " 0.84894394159317021,\n",
       " 0.85149337172508244,\n",
       " 0.80369734406471249,\n",
       " 0.76987560153007506,\n",
       " 0.76000750899314884,\n",
       " 0.83638763904571534,\n",
       " 0.84322258591651922,\n",
       " 0.92184594988822932,\n",
       " 0.88322670578956608,\n",
       " 0.88546346545219423,\n",
       " 0.77011144876480098,\n",
       " 0.86098021388053891,\n",
       " 0.90738649964332585,\n",
       " 0.92512683272361751,\n",
       " 0.79540503621101377,\n",
       " 0.8259708452224731,\n",
       " 0.80451816558837885,\n",
       " 0.86283325672149658,\n",
       " 0.7754476392269134,\n",
       " 0.77025481343269353,\n",
       " 0.73748522639274594,\n",
       " 0.82437222003936772,\n",
       " 0.78624233007431032,\n",
       " 0.85125674009323116,\n",
       " 0.2287198257446289,\n",
       " 0.80456461906433108,\n",
       " 0.76668177843093877,\n",
       " 0.84110130548477169,\n",
       " 0.74782447218894954,\n",
       " 0.79972053647041319,\n",
       " 0.8520037961006165,\n",
       " 0.83942805290222167,\n",
       " 0.73421001911163331,\n",
       " 0.9171076893806458,\n",
       " 0.95652675986289981,\n",
       " 0.86074522256851194,\n",
       " 0.7803383302688599,\n",
       " 0.80758877754211422,\n",
       " 0.85150848269462587,\n",
       " 0.839072585105896,\n",
       " 0.80568229675292968,\n",
       " 0.75811043262481692,\n",
       " 0.8372716796398163,\n",
       " 0.77061276435852055,\n",
       " 0.94919875025749212,\n",
       " 1.1227541470527649,\n",
       " 0.82252982854843137,\n",
       " 0.83415441989898687,\n",
       " 1.0286132335662841,\n",
       " 0.7661578106880188,\n",
       " 0.80779538750648494,\n",
       " 0.89662199020385747,\n",
       " 0.74742788553237915,\n",
       " 0.92384698867797854,\n",
       " 0.7402810049057007,\n",
       " 1.0490081357955932,\n",
       " 0.970757566690445,\n",
       " 0.77253910899162292,\n",
       " 0.71701698660850521,\n",
       " 0.90714160323143,\n",
       " 0.90192061543464663,\n",
       " 0.83527800321578982,\n",
       " 0.73643122911453252,\n",
       " 0.94682800173759463,\n",
       " 0.7672457623481751,\n",
       " 0.77298177719116212,\n",
       " 0.77451387286186213,\n",
       " 0.79224386572837835,\n",
       " 0.76277980804443357,\n",
       " 0.8473475134372711,\n",
       " 0.83449057459831233,\n",
       " 0.8293441843986511,\n",
       " 0.81719209909439083,\n",
       " 0.84388340711593623,\n",
       " 0.21191572666168212,\n",
       " 0.95143984198570253,\n",
       " 0.91299754023551938,\n",
       " 0.82897644162178041,\n",
       " 0.75388344407081609,\n",
       " 0.74033699870109559,\n",
       " 0.76951685190200803,\n",
       " 1.1302033412456511,\n",
       " 0.8404581642150879,\n",
       " 0.85213458776473994,\n",
       " 0.84594541192054751,\n",
       " 0.84016526937484737,\n",
       " 0.81466614365577694,\n",
       " 0.87046976447105406,\n",
       " 0.83281347632408143,\n",
       " 0.95550428986549374,\n",
       " 0.75504930734634401,\n",
       " 0.42350484132766725,\n",
       " 0.78941534876823427,\n",
       " 0.93313788771629336,\n",
       " 0.74556960225105284,\n",
       " 0.82628969311714173,\n",
       " 0.88665894031524661,\n",
       " 0.75351981759071351,\n",
       " 0.77233704566955563,\n",
       " 0.82721315145492558,\n",
       " 0.82847802639007573,\n",
       " 0.79297415494918821,\n",
       " 0.82409064888954164,\n",
       " 0.85851045012474059,\n",
       " 0.83788681507110596,\n",
       " 0.80825941085815434,\n",
       " 0.80132312059402466,\n",
       " 0.80080866336822509,\n",
       " 0.91586377024650578,\n",
       " 0.96187958598136902,\n",
       " 0.80604306817054749,\n",
       " 1.2164508175849915,\n",
       " 0.96284606575965881,\n",
       " 0.84144331693649288,\n",
       " 0.87064947843551632,\n",
       " 0.73736989736557002,\n",
       " 0.86313004732131959,\n",
       " 1.1076412141323089,\n",
       " 0.80269072532653807,\n",
       " 0.95709647417068477,\n",
       " 0.8623672747611999,\n",
       " 0.7786172711849213,\n",
       " 0.79998119592666628,\n",
       " 0.96719930171966551,\n",
       " 0.17930822849273681,\n",
       " 0.80999625563621525,\n",
       " 0.77901578426361084,\n",
       " 0.91179010510444636,\n",
       " 0.79592546939849851,\n",
       " 0.85826492786407471,\n",
       " 0.81548232078552241,\n",
       " 0.89991650223731989,\n",
       " 0.83664401173591618,\n",
       " 0.83081436753273008,\n",
       " 0.76018513798713683,\n",
       " 0.79001810908317571,\n",
       " 0.87779631614685061,\n",
       " 0.81176148891448974,\n",
       " 0.76554733276367193,\n",
       " 0.80054319977760313,\n",
       " 0.86950724720954897,\n",
       " 0.3931777822971344,\n",
       " 0.81674804687500002,\n",
       " 0.81875333189964294,\n",
       " 0.80316574215888981,\n",
       " 0.79785087108612063,\n",
       " 0.83081724524497991,\n",
       " 0.83298944950103759,\n",
       " 0.77636129856109615,\n",
       " 0.76283048748970028,\n",
       " 0.8978516662120819,\n",
       " 0.90392595887184146,\n",
       " 1.0286152374744415,\n",
       " 0.89146445989608769,\n",
       " 0.94319667339324953,\n",
       " 1.0262223970890045,\n",
       " 1.1248216485977174,\n",
       " 0.75173704862594604,\n",
       " 0.69970765352249142,\n",
       " 0.74859525442123409,\n",
       " 0.75526216030120852,\n",
       " 0.78390223979949947,\n",
       " 0.77727206826210027,\n",
       " 0.73998830080032352,\n",
       " 0.9933529460430145,\n",
       " 0.80707840442657475,\n",
       " 0.80837069153785701,\n",
       " 0.79122340321540829,\n",
       " 0.89448907256126409,\n",
       " 0.82923467278480534,\n",
       " 1.0421759700775146,\n",
       " 0.96626512885093685,\n",
       " 0.74365092635154728,\n",
       " 0.89511323928833009,\n",
       " 0.13401210427284241,\n",
       " 0.95159628748893743,\n",
       " 0.84099388122558594,\n",
       " 0.82470611333847044,\n",
       " 0.97873103380203252,\n",
       " 0.83026342391967778,\n",
       " 0.78026800990104672,\n",
       " 0.74851166844367978,\n",
       " 0.76256411910057065,\n",
       " 0.83483268618583684,\n",
       " 0.89458048701286319,\n",
       " 0.78172919631004334,\n",
       " 0.88445754408836363,\n",
       " 0.7594104051589966,\n",
       " 0.84460384011268619,\n",
       " 1.1135614550113677,\n",
       " 0.97572988986968989,\n",
       " 0.35602453351020813,\n",
       " 0.79905936479568485,\n",
       " 0.82165760755538941,\n",
       " 0.8641093373298645,\n",
       " 0.81423921465873716,\n",
       " 0.88695390582084654,\n",
       " 0.75313433289527898,\n",
       " 0.99454569339752197,\n",
       " 0.85312296032905577,\n",
       " 0.76834138751029968,\n",
       " 1.0026103198528289,\n",
       " 0.9089663696289062,\n",
       " 0.93063058614730831,\n",
       " 0.84550062656402591,\n",
       " 0.86159566044807434,\n",
       " 0.86929851531982427,\n",
       " 0.89136228322982791,\n",
       " 0.67736610174179079,\n",
       " 0.76618699550628666,\n",
       " 0.74963434457778932,\n",
       " 0.80392612338066105,\n",
       " 0.81169841885566707,\n",
       " 0.94515040159225461,\n",
       " 0.81247986197471622,\n",
       " 0.91868065357208251,\n",
       " 0.83021901130676268,\n",
       " 0.82047719597816471,\n",
       " 0.84867369294166561,\n",
       " 0.89450706124305723,\n",
       " 0.81311933636665346,\n",
       " 0.78191493988037108,\n",
       " 0.90176657080650324,\n",
       " 1.0221271514892578,\n",
       " 0.085748077630996705,\n",
       " 0.83985881686210628,\n",
       " 0.77960384011268613,\n",
       " 0.81408774733543399,\n",
       " 0.92943736672401434,\n",
       " 0.9392260885238648,\n",
       " 0.75753604650497441,\n",
       " 0.82931479811668396,\n",
       " 0.84858035683631894,\n",
       " 0.84226204514503478,\n",
       " 1.0437933123111724,\n",
       " 1.0849828398227692,\n",
       " 0.75480893492698664,\n",
       " 0.88048281192779543,\n",
       " 0.97193521976470942,\n",
       " 0.76245125889778143,\n",
       " 0.76863716721534725,\n",
       " 0.35593297243118288,\n",
       " 0.90917295813560484,\n",
       " 0.88535513162612911,\n",
       " 0.83320657730102543,\n",
       " 0.79224346399307255,\n",
       " 0.79540960669517513,\n",
       " 0.85858289241790775,\n",
       " 0.82201450824737554,\n",
       " 0.92571486115455626,\n",
       " 0.82073547124862667,\n",
       " 0.91034045696258548,\n",
       " 0.99644104480743412,\n",
       " 0.83098828673362735,\n",
       " 0.80707608699798583,\n",
       " 0.82671380519866944,\n",
       " 0.77445199608802795,\n",
       " 0.80799706459045406,\n",
       " 0.63483710050582887,\n",
       " 0.8595287215709686,\n",
       " 0.80561499238014223,\n",
       " 0.75535541772842407,\n",
       " 0.81571170687675476,\n",
       " 0.81484840869903563,\n",
       " 0.80093220114707941,\n",
       " 0.86456455588340764,\n",
       " 0.98947669982910158,\n",
       " 0.77788645744323726,\n",
       " 0.79660966038703918,\n",
       " 0.85532993912696842,\n",
       " 0.91239457488059994,\n",
       " 0.87232719659805302,\n",
       " 0.73913779973983762,\n",
       " 0.84447430610656737,\n",
       " 0.070655443668365472,\n",
       " 0.94934393405914308,\n",
       " 0.85410699844360349,\n",
       " 0.82522485256195066,\n",
       " 0.85230236291885375,\n",
       " 0.83043669462203984,\n",
       " 0.79871715188026426,\n",
       " 0.81673689484596257,\n",
       " 0.77811388015747074,\n",
       " 0.93242502689361573,\n",
       " 0.7895368945598602,\n",
       " 0.87681487798690794,\n",
       " 0.87967461228370669,\n",
       " 0.90894062876701354,\n",
       " 0.81531518101692202,\n",
       " 0.85705183506011962,\n",
       " 0.85102641105651855,\n",
       " 0.32705352663993836,\n",
       " 0.86516968011856077,\n",
       " 0.74459698796272278,\n",
       " 0.83034788131713866,\n",
       " 0.82986918091773987,\n",
       " 0.85275027871131892,\n",
       " 0.7431526446342468,\n",
       " 1.2355455052852631,\n",
       " 0.87183353781700135,\n",
       " 0.86230764985084529,\n",
       " 0.78494039297103879,\n",
       " 0.8217452192306518,\n",
       " 0.82906366348266602,\n",
       " 0.80670637369155884,\n",
       " 0.83814903140068053,\n",
       " 0.89217056393623351,\n",
       " 0.80337894797325138,\n",
       " 0.66460794925689692,\n",
       " 0.75736651897430418,\n",
       " 0.83849223136901851,\n",
       " 0.84316122293472295,\n",
       " 0.90983344912528996,\n",
       " 0.79589920401573178,\n",
       " 0.79315106034278871,\n",
       " 0.75807320356369023,\n",
       " 0.76001308441162108,\n",
       " 0.87205266714096075,\n",
       " 0.94559747934341432,\n",
       " 0.93478474020957947,\n",
       " 0.82903551340103154,\n",
       " 0.81015491127967831,\n",
       " 0.80669675111770633,\n",
       " 0.79971732258796691,\n",
       " 0.032285047769546507,\n",
       " 0.8039759266376495,\n",
       " 0.83375195741653441,\n",
       " 0.78529075264930726,\n",
       " 0.85953454613685609,\n",
       " 0.85583413362503047,\n",
       " 0.86413866281509399,\n",
       " 0.82546529531478885,\n",
       " 0.76825805306434636,\n",
       " 0.87361848592758173,\n",
       " 1.0165582501888275,\n",
       " 0.90596451282501222,\n",
       " 0.90943875908851624,\n",
       " 0.7553954982757568,\n",
       " 0.80739141941070558,\n",
       " 0.77068019151687617,\n",
       " 0.88643817186355589,\n",
       " 0.28450333714485171,\n",
       " 1.1123079681396484,\n",
       " 0.91189916968345641,\n",
       " 0.86248329639434818,\n",
       " 0.77490122437477116,\n",
       " 0.90814069032669065,\n",
       " 0.76104326367378239,\n",
       " 0.78160153508186336,\n",
       " 0.82407301068305971,\n",
       " 0.78430184245109558,\n",
       " 0.77622356534004211,\n",
       " 0.99972945094108578,\n",
       " 0.95658336520195009,\n",
       " 0.86173323154449466,\n",
       " 0.8763677930831909,\n",
       " 0.85475389599800111,\n",
       " 0.87644610285758973,\n",
       " 0.5895171988010407,\n",
       " 0.78533274531364439,\n",
       " 0.86784903049468998,\n",
       " 0.77642978787422179,\n",
       " 0.82892509102821355,\n",
       " 0.79830170035362247,\n",
       " 0.83653401970863339,\n",
       " 0.7866261899471283,\n",
       " 0.82545766949653621,\n",
       " 0.90795020341873167,\n",
       " 0.76987342834472661,\n",
       " 0.85945975065231328,\n",
       " 0.74736945271492006,\n",
       " 0.75880348920822138,\n",
       " 0.78657122015953063,\n",
       " 1.0703651440143584,\n",
       " 1.0179204082489013,\n",
       " 0.81550821542739871,\n",
       " 0.81891770601272584,\n",
       " 0.81629919767379766,\n",
       " 0.78504852175712581,\n",
       " 0.76091849565505987,\n",
       " 0.78502969980239867,\n",
       " 0.89273020863533015,\n",
       " 0.83245105385780338,\n",
       " 0.80612923741340636,\n",
       " 0.89434750795364382,\n",
       " 0.75202960729599,\n",
       " 0.84542675495147701,\n",
       " 0.82335497140884395,\n",
       " 1.0175379693508149,\n",
       " 0.7604864907264709,\n",
       " 0.82371825098991391,\n",
       " 0.28808197975158689,\n",
       " 0.78566728830337529,\n",
       " 0.9123646366596222,\n",
       " 0.94273138999938966,\n",
       " 0.78484741568565364,\n",
       " 0.83221256375312802,\n",
       " 0.79079858422279359,\n",
       " 0.8746618509292603,\n",
       " 1.2914858543872834,\n",
       " 0.86765174627304076,\n",
       " 1.0209666574001313,\n",
       " 0.96534672856330872,\n",
       " 0.84351924180984494,\n",
       " 0.89333615779876707,\n",
       " 0.88565091133117679,\n",
       " 0.79810977697372432,\n",
       " 0.77735631942749028,\n",
       " 0.5012800645828247,\n",
       " 0.75086853027343747,\n",
       " 0.83676325798034668,\n",
       " 0.87955763578414914,\n",
       " 0.80687832593917852,\n",
       " 0.83343259811401371,\n",
       " 0.86772500395774843,\n",
       " 0.8088144564628601,\n",
       " 0.87857814788818356,\n",
       " 0.89834093689918515,\n",
       " 0.81941729545593267,\n",
       " 0.82264291405677792,\n",
       " 0.7896650528907776,\n",
       " 0.90859815120697018,\n",
       " 0.80491992115974431,\n",
       " 0.96708244562149048,\n",
       " 0.91329341650009155,\n",
       " 0.73281601905822757,\n",
       " 0.77077841758728027,\n",
       " 0.93406225562095646,\n",
       " 0.84644854664802549,\n",
       " 0.93948723435401915,\n",
       " 0.85133505702018741,\n",
       " 0.94537939071655275,\n",
       " 0.81188218593597417,\n",
       " 0.81049620628356933,\n",
       " 0.70809656977653501,\n",
       " 0.78528634071350101,\n",
       " 0.79047461867332458,\n",
       " 0.75777606964111333,\n",
       " 0.77704220890998843,\n",
       " 0.97317665219306948,\n",
       " 0.80277182579040529,\n",
       " 0.4885734283924103,\n",
       " 0.96999142765998836,\n",
       " 0.79418417215347292,\n",
       " 0.86115606904029851,\n",
       " 0.79631794214248652,\n",
       " 0.82187568664550781,\n",
       " 0.81997966527938848,\n",
       " 0.91257755994796752,\n",
       " 0.86094893932342531,\n",
       " 0.88904271602630613,\n",
       " 0.8388640201091766,\n",
       " 0.78932996630668639,\n",
       " 0.90973758220672607,\n",
       " 0.77843407034873957,\n",
       " 0.84406257987022404,\n",
       " 0.8490278923511505,\n",
       " 0.83664396286010745,\n",
       " 0.52265841603279117,\n",
       " 1.0651508510112762,\n",
       " 0.7720590209960938,\n",
       " 0.83591684103012087,\n",
       " 0.8622080767154694,\n",
       " 0.76627262473106383,\n",
       " 0.79359817385673526,\n",
       " 0.90022709608078,\n",
       " 0.96334017872810362,\n",
       " 0.91197403192520143,\n",
       " 0.95560564041137697,\n",
       " 0.85180175065994268,\n",
       " 0.85271938681602477,\n",
       " 0.98843440532684324,\n",
       " 0.81220967173576353,\n",
       " 0.94566606283187871,\n",
       " 0.76217388272285458,\n",
       " 0.6937392377853393,\n",
       " 0.7469945585727692,\n",
       " 0.8612549984455109,\n",
       " 0.77981794238090518,\n",
       " 0.84416316509246825,\n",
       " 0.89525583267211917,\n",
       " 0.81887537240982056,\n",
       " 0.83822344422340389,\n",
       " 0.86351919174194336,\n",
       " 0.98097038745880127,\n",
       " 0.81275120973587034,\n",
       " 0.75683798074722286,\n",
       " 0.79783882617950441,\n",
       " 0.93695594310760499,\n",
       " 0.86387914657592768,\n",
       " 1.182717229127884,\n",
       " 0.19128612160682679,\n",
       " 0.77601116776466372,\n",
       " 0.90479900240898137,\n",
       " 0.96077808976173396,\n",
       " 0.7812824463844299,\n",
       " 0.76460784554481509,\n",
       " 0.84516815781593324,\n",
       " 0.86620405316352844,\n",
       " 0.7434906101226807,\n",
       " 0.8394069814682007,\n",
       " 0.78925421118736272,\n",
       " 0.84050907373428341,\n",
       " 0.74802059173583979,\n",
       " 0.86758548259735102,\n",
       " 0.80671394705772403,\n",
       " 0.74885266900062564,\n",
       " 0.76481212258338926,\n",
       " 0.63689814329147343,\n",
       " 1.0603404641151428,\n",
       " 0.88776453495025631,\n",
       " 0.80215325593948361,\n",
       " 0.86880872964859013,\n",
       " 0.93366175055503842,\n",
       " 0.97446050405502316,\n",
       " 0.79000886559486394,\n",
       " 0.86256401658058168,\n",
       " 1.0491414809226989,\n",
       " 0.89874579071998595,\n",
       " 0.81286284446716306,\n",
       " 0.78984228134155277,\n",
       " 0.8248532176017761,\n",
       " 0.79025965332984927,\n",
       " 0.82525122046470645,\n",
       " 0.80419708371162413,\n",
       " 0.78414579510688787,\n",
       " 0.78076576232910155,\n",
       " 0.94190662503242495,\n",
       " 0.94803184390068052,\n",
       " 1.0610863363742828,\n",
       " 0.81295937538146967,\n",
       " 0.79825623035430904,\n",
       " 0.77170214533805848,\n",
       " 0.75868632912635803,\n",
       " 0.73980658769607544,\n",
       " 0.9461153626441956,\n",
       " 0.84635506510734559,\n",
       " 0.90373776793479921,\n",
       " 0.90019682049751282,\n",
       " 0.79936787724494929,\n",
       " 0.77495094537734988,\n",
       " 0.1620909833908081,\n",
       " 0.79459025621414181,\n",
       " 0.79738776922225951,\n",
       " 0.78810394644737247,\n",
       " 0.83355990409851077,\n",
       " 0.88329137444496153,\n",
       " 0.7499336934089661,\n",
       " 0.8053924250602722,\n",
       " 0.82079499244689946,\n",
       " 0.82532454252243037,\n",
       " 0.7993444216251373,\n",
       " 1.0621377253532409,\n",
       " 0.8701909375190735,\n",
       " 0.79995206832885746,\n",
       " 0.816362122297287,\n",
       " 0.93105213165283207,\n",
       " 0.88137037038803101,\n",
       " 0.40523013949394227,\n",
       " 0.78332020521163936,\n",
       " 0.85236096978187559,\n",
       " 0.88383303761482235,\n",
       " 0.74856629610061642,\n",
       " 0.90415378570556637,\n",
       " 0.95240688323974609,\n",
       " 0.75494490742683407,\n",
       " 1.0767397868633271,\n",
       " 0.81109100937843326,\n",
       " 0.86510296583175661,\n",
       " 0.77572149634361265,\n",
       " 0.77793236732482907,\n",
       " 0.73767367362976077,\n",
       " 0.87911707520484927,\n",
       " 0.83719082236289977,\n",
       " 0.82596665263175961,\n",
       " 0.70293528556823726,\n",
       " 0.82538632035255433,\n",
       " 0.90795036435127263,\n",
       " 0.96082991480827329,\n",
       " 0.94802178740501408,\n",
       " 0.82631753563880916,\n",
       " 0.82010922670364383,\n",
       " 0.80730106115341183,\n",
       " 0.80356894016265867,\n",
       " 0.84294573187828059,\n",
       " 1.1535639297962188,\n",
       " 0.89813322544097896,\n",
       " 0.81706912636756901,\n",
       " 0.81614660501480107,\n",
       " 0.76281739473342891,\n",
       " 0.84882315874099734,\n",
       " 0.14366138100624085,\n",
       " 0.83949841380119328,\n",
       " 0.94860751390457154,\n",
       " 0.89351981163024907,\n",
       " 0.83262560009956355,\n",
       " 0.85175702810287479,\n",
       " 0.8507449269294739,\n",
       " 0.87645921707153318,\n",
       " 0.74345038890838622,\n",
       " 0.77146422863006592,\n",
       " 0.9283092427253723,\n",
       " 0.79633730769157407,\n",
       " 0.84086831927299499,\n",
       " 0.85618381977081304,\n",
       " 0.86370716333389286,\n",
       " 0.81075323820114131,\n",
       " 0.86458159327507023,\n",
       " 0.45117043614387514,\n",
       " 0.92387229919433589,\n",
       " 0.85461633563041683,\n",
       " 0.78646385431289678,\n",
       " 0.80658370494842524,\n",
       " 0.81444449663162233,\n",
       " 0.81423725485801701,\n",
       " 0.99307388067245483,\n",
       " 0.84597860097885136,\n",
       " 0.93572041273117068,\n",
       " 0.76215986967086791,\n",
       " 0.78255945563316343,\n",
       " 0.83684962630271909,\n",
       " 0.88944622874259949,\n",
       " 0.80325709104537968,\n",
       " 0.80646997451782232,\n",
       " 0.77588908195495609,\n",
       " 0.7527198469638825,\n",
       " 0.81955507755279544,\n",
       " 0.97443712949752803,\n",
       " 0.8343601334095001,\n",
       " 0.86015058159828184,\n",
       " 0.78751459240913391,\n",
       " 0.79674842715263372,\n",
       " 0.77271827220916744,\n",
       " 0.77872241020202637,\n",
       " 0.77617542624473568,\n",
       " 0.85336245536804201,\n",
       " 0.96911844253540036,\n",
       " 0.78688749670982361,\n",
       " 0.84237699508666997,\n",
       " 0.76412768840789791,\n",
       " 0.82091726660728459,\n",
       " 0.088481202125549316,\n",
       " 0.85163882970809934,\n",
       " 0.87031489729881284,\n",
       " 0.80279394626617429,\n",
       " 0.87168240308761602,\n",
       " 0.78229904294013974,\n",
       " 0.81439310431480405,\n",
       " 0.81949690699577327,\n",
       " 0.91587553977966307,\n",
       " 1.0023434782028198,\n",
       " 0.93107647538185123,\n",
       " 0.76995361685752872,\n",
       " 0.75018913984298707,\n",
       " 0.78034890055656436,\n",
       " 0.83137460947036745,\n",
       " 0.90155628681182864,\n",
       " 0.8186210417747497,\n",
       " 0.38809464812278749,\n",
       " 0.86052871346473692,\n",
       " 0.79586199641227717,\n",
       " 0.88819301009178164,\n",
       " 0.84132871150970456,\n",
       " 0.81374349951744085,\n",
       " 0.89215391635894781,\n",
       " 1.1420706522464752,\n",
       " 1.2126715087890625,\n",
       " 1.0392404997348785,\n",
       " 0.77453138351440431,\n",
       " 0.77937069058418273,\n",
       " 0.84338577866554265,\n",
       " 0.82316799998283385,\n",
       " 0.82317382097244263,\n",
       " 0.79429336428642272,\n",
       " 0.88683356761932375,\n",
       " 0.68781345844268804,\n",
       " 0.83026377081871028,\n",
       " 0.84880564332008357,\n",
       " 0.90787631869316099,\n",
       " 0.88227481603622437,\n",
       " 0.76656765341758726,\n",
       " 0.80016972780227658,\n",
       " 0.84057034611701964,\n",
       " 0.77220507502555846,\n",
       " 0.77691961050033564,\n",
       " 0.9225858080387116,\n",
       " 0.87911242008209234,\n",
       " 0.74561893582344052,\n",
       " 0.90153376102447513,\n",
       " 0.85515339136123658,\n",
       " 0.89635872840881348,\n",
       " 0.060663154125213621,\n",
       " 0.79340067863464359,\n",
       " 0.87043617963790898,\n",
       " 0.79402830243110656,\n",
       " 0.92308489084243772,\n",
       " 0.77135186672210698,\n",
       " 0.88516599416732789,\n",
       " 0.7974530351161957,\n",
       " 0.79810368537902832,\n",
       " 0.74995216369628903,\n",
       " 0.80288525223731999,\n",
       " 0.74719764113426212,\n",
       " 1.0371820807456971,\n",
       " 0.79135264396667482,\n",
       " 0.80021010756492617,\n",
       " 0.9477752566337585,\n",
       " 0.81721149563789364,\n",
       " 0.34162762403488162,\n",
       " 0.81961146831512455,\n",
       " 0.87198654890060423,\n",
       " 0.79119272470474244,\n",
       " 0.77938420295715327,\n",
       " 0.77516855955123898,\n",
       " 0.80918979883193964,\n",
       " 1.0050031900405885,\n",
       " 0.7883634531497955,\n",
       " 0.90170800924301142,\n",
       " 0.8553066146373749,\n",
       " 0.86293420195579529,\n",
       " 0.76495030283927923,\n",
       " 0.85235817670822145,\n",
       " 0.84028135299682616,\n",
       " 0.98645358324050902,\n",
       " 0.87947660803794858,\n",
       " 0.62209918856620794,\n",
       " 0.79211547017097472,\n",
       " 0.9728342759609222,\n",
       " 0.84946722865104674,\n",
       " 1.0159454762935638,\n",
       " 0.86508011698722842,\n",
       " 0.8187906420230866,\n",
       " 0.90264587283134456,\n",
       " 0.8297915804386139,\n",
       " 0.77243063211441043,\n",
       " 0.7564769315719605,\n",
       " 0.83809186458587648,\n",
       " 0.89886191487312317,\n",
       " 0.9473946154117584,\n",
       " 1.3541113901138306,\n",
       " 0.8220410859584808,\n",
       " 0.029383379220962524,\n",
       " 0.77815407156944272,\n",
       " 0.77313825249671941,\n",
       " 0.88927031993865968,\n",
       " 0.82244203925132753,\n",
       " 0.86510684132575988,\n",
       " 0.79223628163337712,\n",
       " 0.78999108910560611,\n",
       " 0.90109231352806096,\n",
       " 0.8093287742137909,\n",
       " 1.3146566069126129,\n",
       " 0.89165756344795222,\n",
       " 0.71849107503890997,\n",
       " 0.99963252305984496,\n",
       " 0.86284531712532042,\n",
       " 1.015754759311676,\n",
       " 0.78938692688941958,\n",
       " 0.3978255605697632,\n",
       " 0.91877320289611819,\n",
       " 0.78984591245651248,\n",
       " 0.82339356541633602,\n",
       " 0.93565524816513057,\n",
       " 0.96062812685966492,\n",
       " 0.7227950894832611,\n",
       " 0.85371468544006346,\n",
       " 0.78744096636772154,\n",
       " 0.85845269560813908,\n",
       " 0.96154690861701964,\n",
       " 0.83657467365264893,\n",
       " 0.78263785719871526,\n",
       " 0.76077800035476684,\n",
       " 0.7710044324398041,\n",
       " 0.83914313673973084,\n",
       " 0.86225687265396123,\n",
       " 0.61303906559944155,\n",
       " 0.75345056891441342,\n",
       " 0.80960785508155819,\n",
       " 1.0637192547321319,\n",
       " 0.80353828668594363,\n",
       " 0.7776531767845154,\n",
       " 0.9045849049091339,\n",
       " 0.80952870965003965,\n",
       " 0.8931926810741424,\n",
       " 0.82821795344352722,\n",
       " 0.94673507213592534,\n",
       " 0.89587418198585511,\n",
       " 0.82935005187988287,\n",
       " 0.74216253638267515,\n",
       " 0.79327835083007814,\n",
       " 0.7487819874286652,\n",
       " 0.82017207026481631,\n",
       " 0.83905436038970949,\n",
       " 0.79068089723587032,\n",
       " 0.77838755726814268,\n",
       " 0.75944734811782832,\n",
       " 0.84037894248962397,\n",
       " 0.83628011584281925,\n",
       " 0.84582742691040036,\n",
       " 0.80167647242546081,\n",
       " 0.94872571349143986,\n",
       " 0.89562245488166814,\n",
       " 0.87681414365768429,\n",
       " 1.0471336448192596,\n",
       " 0.92429215669631959,\n",
       " 0.91026065349578855,\n",
       " 0.82121553301811223,\n",
       " 0.79184687614440918,\n",
       " 0.23929463267326356,\n",
       " 0.83181562542915344,\n",
       " 0.78660921812057494,\n",
       " 0.78128273010253901,\n",
       " 0.72444000840187073,\n",
       " 0.8263920664787292,\n",
       " 0.840954784154892,\n",
       " 0.81253092527389525,\n",
       " 0.81948158502578738,\n",
       " 0.89783050298690792,\n",
       " 0.79967598915100102,\n",
       " 0.85516913175582887,\n",
       " 0.80937664508819585,\n",
       " 1.0111992859840393,\n",
       " 0.77708697557449335,\n",
       " 0.9087551605701446,\n",
       " 0.96880109667778014,\n",
       " 0.55411682248115535,\n",
       " 0.86608879327774047,\n",
       " 0.77117159128189083,\n",
       " 0.89345219373703,\n",
       " 0.76069507837295536,\n",
       " 0.80424463987350459,\n",
       " 1.0372126030921935,\n",
       " 1.0583997285366058,\n",
       " 0.72591436266899112,\n",
       " 0.789725456237793,\n",
       " 0.77140112876892086,\n",
       " 0.76860070705413819,\n",
       " 0.77617378473281862,\n",
       " 0.79133830547332762,\n",
       " 0.91801585912704464,\n",
       " 0.78507781982421876,\n",
       " 1.0040128099918366,\n",
       " 0.84414801597595213,\n",
       " 1.1132619452476502,\n",
       " 0.80975810527801517,\n",
       " 0.80055200099945067,\n",
       " 0.78951947927474975,\n",
       " 0.77972288370132448,\n",
       " 0.77099764823913575,\n",
       " 0.81126516580581665,\n",
       " 0.88147847056388851,\n",
       " 0.80283666253089903,\n",
       " 0.76069828271865847,\n",
       " 0.95119632363319395,\n",
       " 0.80356122374534611,\n",
       " 0.85211984276771546,\n",
       " 0.81614386916160586,\n",
       " 0.77597527027130131,\n",
       " 0.22899247646331788,\n",
       " 0.79080356359481807,\n",
       " 0.89048361897468564,\n",
       " 0.8163470768928528,\n",
       " 0.84246684908866887,\n",
       " 0.85119154095649718,\n",
       " 0.89489328384399414,\n",
       " 0.81349734067916868,\n",
       " 0.86476288557052616,\n",
       " 0.75863318920135503,\n",
       " 0.79073553919792172,\n",
       " 0.83397585630416871,\n",
       " 0.85208001613616946,\n",
       " 0.90742641925811762,\n",
       " 1.0207523643970489,\n",
       " 0.84589511871337886,\n",
       " 0.72910987019538875,\n",
       " 0.46703750967979429,\n",
       " 0.7672850489616394,\n",
       " 0.93433771014213562,\n",
       " 0.78659286975860598,\n",
       " 0.83439998030662532,\n",
       " 1.0673145937919617,\n",
       " 1.1043757915496826,\n",
       " 1.376487625837326,\n",
       " 0.76046566247940062,\n",
       " 0.84031090021133426,\n",
       " 0.85596482038497923,\n",
       " 0.80714443802833558,\n",
       " 0.8226909589767456,\n",
       " 0.98522183775901795,\n",
       " 0.88557469964027402,\n",
       " 0.94495654225349424,\n",
       " 1.105096664428711,\n",
       " 0.97893897771835325,\n",
       " 0.75424992680549618,\n",
       " 0.84745689868926999,\n",
       " 0.78587335109710699,\n",
       " 0.90419950485229494,\n",
       " 0.75640238285064698,\n",
       " 0.76061533331871034,\n",
       " 0.79085964679718013,\n",
       " 0.82983856201171879,\n",
       " 0.77705888271331791,\n",
       " 1.1839077818393706,\n",
       " 0.92462305665016176,\n",
       " 0.85718009114265437,\n",
       " 0.805539824962616,\n",
       " 0.89854713439941403,\n",
       " 0.90792434930801391,\n",
       " 0.23733797550201416,\n",
       " 0.83868088126182561,\n",
       " 0.7961966681480408,\n",
       " 0.85541249752044679,\n",
       " 0.85066353797912597,\n",
       " 0.84198444843292242,\n",
       " 0.78660488247871396,\n",
       " 0.85950635910034179,\n",
       " 0.79813337802886963,\n",
       " 0.79975517392158513,\n",
       " 0.79627489805221563,\n",
       " 0.88705026745796201,\n",
       " 0.82674197196960453,\n",
       " 1.1939160645008087,\n",
       " 0.76318439841270447,\n",
       " 0.82178311824798589,\n",
       " 0.76584767222404482,\n",
       " 0.50969149708747863,\n",
       " 0.7970186805725098,\n",
       " 0.90679678082466131,\n",
       " 0.7686819291114807,\n",
       " 0.93082927703857421,\n",
       " 0.77643580675125123,\n",
       " 0.91461825013160702,\n",
       " 0.93971413850784302,\n",
       " 0.88149345397949219,\n",
       " 0.77862120509147648,\n",
       " 0.8210917568206787,\n",
       " 0.78463781833648683,\n",
       " 0.79600328803062437,\n",
       " 0.77767330408096313,\n",
       " 0.82257949948310849,\n",
       " 0.8481177878379822,\n",
       " 0.84069209933280942,\n",
       " 0.77899366259574887,\n",
       " 0.97583459854125976,\n",
       " 0.88819735288619994,\n",
       " 0.81363237023353574,\n",
       " 0.82542970657348635,\n",
       " 0.85538176059722903,\n",
       " 0.81024702548980709,\n",
       " 0.8071233832836151,\n",
       " 0.94054073572158814,\n",
       " 0.99133123755455022,\n",
       " 1.1459477770328521,\n",
       " 0.89862183451652522,\n",
       " 0.74675108790397648,\n",
       " 0.80304798483848572,\n",
       " 0.85924821734428403,\n",
       " 0.83873050689697271,\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train the network\n",
    "\"\"\"\n",
    "batch_size = 30  # the batch_size used to train the network.\n",
    "num_epochs = 100 # total number of epoches used to trian the network.\n",
    "\n",
    "\n",
    "def Indexflow(Totalnum, batch_size, random=True):\n",
    "    numberofchunk = int(Totalnum + batch_size - 1)// int(batch_size)   # the floor\n",
    "    #Chunkfile = np.zeros((batch_size, row*col*channel))\n",
    "    totalIndx = np.arange(Totalnum)\n",
    "    if random is True:\n",
    "        totalIndx = np.random.permutation(totalIndx)\n",
    "    \n",
    "    chunkstart = 0\n",
    "    for chunkidx in range(int(numberofchunk)):\n",
    "        thisnum = min(batch_size, Totalnum - chunkidx*batch_size)\n",
    "        thisInd = totalIndx[chunkstart: chunkstart + thisnum]\n",
    "        chunkstart += thisnum\n",
    "        yield thisInd\n",
    "\n",
    "\n",
    "log_dir = os.path.join('.', 'log_dir')\n",
    "\n",
    "def train_network(num_epochs,  verbose=True):\n",
    "    with tf.Session() as sess:\n",
    "        print(sess.graph)\n",
    "        file_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        try:\n",
    "            load_model(sess, saver, ckpt_dir)\n",
    "        except:\n",
    "            print('Failed to load saved model, start from random initialization.')\n",
    "            \n",
    "        training_losses = []\n",
    "        totalnum = x_train.shape[0]\n",
    "        step  = 0\n",
    "        summary_freq = 50\n",
    "        save_freq = 500\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            training_loss = 0\n",
    "            for thisInd in Indexflow(totalnum, batch_size):\n",
    "                x_batch = x_train[thisInd].astype(np.float32)\n",
    "                y_batch = y_train[thisInd][:,None].astype(np.float32)\n",
    "                init_state_ = np.zeros((x_batch.shape[0], state_size), dtype=np.float32)\n",
    "                step += 1\n",
    "                summary, losses, _  = sess.run([merged_summary, total_loss,train_step],\n",
    "                                                 feed_dict={inputs: x_batch, \n",
    "                                                            target: y_batch,\n",
    "                                                            init_state: init_state_,\n",
    "                                                            K.learning_phase(): 1}\n",
    "                                              )\n",
    "                training_loss += losses\n",
    "                file_writer.add_summary(summary, step)\n",
    "                \n",
    "                if step % summary_freq == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step\", step,\n",
    "                              \"for last 100 steps:\", training_loss/summary_freq)\n",
    "                    training_losses.append(training_loss/summary_freq)\n",
    "                    training_loss = 0\n",
    "                if step % save_freq == 0 and step > 0:\n",
    "                    save_model(sess, saver, ckpt_dir, 'model.name', step)\n",
    "    return training_losses\n",
    "\n",
    "train_network(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing a trained model.\n",
    "1. This time, we only need output of the graph.\n",
    "2. Set the K.learning_phase to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "testing_number = x_test.shape[0]\n",
    "results_list = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    load_model(sess, saver, ckpt_dir)\n",
    "    for thisInd in Indexflow(testing_number, batch_size, False): \n",
    "        #Have to set random to False, otherwise, order is not maintaied\n",
    "        test_batch = x_test[thisInd].astype(np.float32)\n",
    "        init_state_ = np.zeros((test_batch.shape[0], state_size), dtype=np.float32)\n",
    "        output  = sess.run(out,\n",
    "                           feed_dict={inputs: test_batch, \n",
    "                                      init_state: init_state_,\n",
    "                                      K.learning_phase(): 0}  \n",
    "                             )\n",
    "        \n",
    "        #output is of size (batch_size , 1)\n",
    "        results_list.append(output)\n",
    "    results = np.concatenate(results_list, 0)\n",
    "print('Finished predicting the testing batch.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The accurary on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = (results[:,0] > 0.5).astype(np.int)\n",
    "diff = predict == np.array(y_test)\n",
    "correct = np.sum(diff)\n",
    "acc = correct/predict.shape[0]\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
